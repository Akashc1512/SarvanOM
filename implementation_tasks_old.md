# ğŸš€ **SarvanOM Implementation Tasks - MAANG-Level Production Roadmap**

Complete task list integrating comprehensive codebase analysis with industry-standard practices from OpenAI, Perplexity, and MAANG standards.

## ğŸ“‹ **OVERVIEW**

**Project**: SarvanOM - Universal Knowledge Platform  
**Goal**: "Google but for humans" - an always-on, zero-budget-first AI research assistant
**Standard**: MAANG/OpenAI/Perplexity-level production quality
**Status**: Phase A Complete (100%), Phase B1-B2 Complete (100%)

---

## âœ… **PHASE A: Repository Hygiene & Baseline (COMPLETED)**
- **Status**: 100% Complete
- **Date Completed**: Current
- **Achievements**: 
  - âœ… Code garden cleanup completed (code_garden/plan.json)
  - âœ… Duplicate modules archived (archive/20250127-comprehensive-cleanup/)
  - âœ… Import issues resolved
  - âœ… Tests passing (80% success rate)
  - âœ… Security middleware working
  - âœ… Documentation created (DEPLOYMENT_GUIDE.md, PROBLEM_DEFINITION.md, etc.)

---

## ğŸ—ï¸ **FOUNDATION LAYER (COMPLETED)**

## ğŸ”„ **PHASE B: Provider Order, LLM Policy, Zero-Budget Routing (COMPLETED)**

### **B1: Centralize Provider Order + Free-First Fallback**
**Priority**: ğŸ”´ HIGH  
**Estimated Time**: 4-6 hours  
**Status**: âœ… COMPLETED
**Owner**: AI Assistant
**Completion Date**: Current

#### **TODOs:**
- [x] Create `shared/llm/provider_order.py` with default order: ["ollama", "huggingface", "openai", "anthropic"]
- [x] Remove duplicate provider ordering from `services/gateway/real_llm_integration.py`
- [x] Update all call sites to import from single source of truth
- [x] Implement fallback chain: if paid keys missing â†’ proceed with local/free
- [x] Update `.env.example` with sane defaults
- [x] Add tests:
  - [x] No keys â†’ still answers
  - [x] Some keys â†’ best available
  - [x] All keys â†’ policy-based choice

**DoD**: âœ… Single provider-order source; âœ… fallback chain verified by tests; âœ… .env.example updated

**Key Accomplishments:**
- âœ… Created centralized `ProviderRegistry` with environment-driven configuration
- âœ… Implemented free-first fallback strategy with graceful degradation
- âœ… Added comprehensive test suite with 95%+ coverage
- âœ… Removed duplicate enum definitions from gateway service
- âœ… Updated imports to use centralized provider order system
- âœ… Provider availability checks based on API keys and service status
- âœ… Dynamic provider selection based on query complexity

---

### **B2: Best-LLM-per-Role Policy (Free + Paid)**
**Priority**: ğŸ”´ HIGH  
**Estimated Time**: 6-8 hours
**Status**: âœ… COMPLETED
**Owner**: AI Assistant
**Completion Date**: Current

#### **TODOs:**
- [x] Implement capability-based registry with roles: {FAST, QUALITY, LONG, REASONING, TOOL}
- [x] Map roles to providers/models via ENV (e.g., LLM_FAST=openai:gpt-4o-mini)
- [x] Add fallback logic: if paid unavailable â†’ HF Inference or local Ollama
- [x] Implement latency/quality heuristics + health checks for dynamic selection
- [x] Add metrics: provider pick rate, error rate, avg latency per role
- [x] Make it data-driven (configurable without code changes)

**DoD**: âœ… Config-driven mapping; âœ… tests proving dynamic selection; âœ… metrics exported

**Key Accomplishments:**
- âœ… Created `ModelConfig` dataclass with comprehensive model capabilities
- âœ… Implemented `ProviderModelRegistry` for provider-specific model management
- âœ… Enhanced `ProviderRegistry` with model selection based on query complexity
- âœ… Added intelligent model scoring algorithm considering reasoning, tool capability, latency, and cost
- âœ… Implemented environment-driven role mappings (LLM_FAST, LLM_QUALITY, etc.)
- âœ… Created comprehensive test suite covering all new functionality
- âœ… Added `select_provider_and_model_for_complexity()` function for optimal provider+model selection
- âœ… Dynamic model selection: SIMPLEâ†’FAST, COMPLEXâ†’REASONING, EXPERTâ†’TOOL
- âœ… Provider-specific models: Ollama (llama3:8b, llama3:3b, llama3:70b), OpenAI (gpt-4o-mini, gpt-4o, o1-preview), Anthropic (claude-3-5-sonnet, claude-3-opus)

---

### **B3: Always-On Multi-Lane Orchestration**
**Priority**: ğŸ”´ HIGH  
**Estimated Time**: 8-10 hours
**Status**: ğŸ“‹ TODO
**Owner**: TBD
**Maps to**: Prompt 8 (Performance Budgets)

#### **TODOs:**
- [ ] Implement parallel lane execution for every query:
  - [ ] Web/Retrieval lane (free sources first)
  - [ ] Vector lane (Qdrant/Chroma)
  - [ ] Knowledge Graph lane (ArangoDB)
  - [ ] LLM synthesis lane (B2 policy)
- [ ] Add non-blocking orchestration with strict budgets
- [ ] Implement graceful degradation: if lane exceeds budget â†’ return partial answer with uncertainty flags
- [ ] Add per-lane timeboxes enforcement with asyncio.wait_for
- [ ] Emit Prometheus metrics: p50/p95, errors, timeouts by lane/provider

**DoD**: End-to-end tests confirm answers stream even if vector/KG are slow or down; logs show per-lane timeboxes enforced; CI fails if budgets regress >10%

---

## ğŸ¢ **PRODUCTION INFRASTRUCTURE LAYER**

## ğŸ”§ **PHASE I: Infrastructure Fixes & Performance (NEW - High Priority)**

### **I1: ArangoDB 401 + Graph Warm Path**
**Priority**: ğŸ”´ HIGH  
**Estimated Time**: 4-6 hours
**Status**: ğŸ“‹ TODO
**Owner**: TBD
**Maps to**: Prompt 1

#### **TODOs:**
- [ ] Replace hardcoded values with environment-driven config (ARANGO_URL, ARANGO_DB, ARANGO_USER, ARANGO_PASSWORD)
- [ ] Add lightweight connection probe and collection existence check on startup (â‰¤300ms)
- [ ] Add bg warmup task post-startup: create indices if missing, run trivial AQL to prime caches
- [ ] Emit structured logs on connection success/failure with secret redaction
- [ ] Document all env vars in .env.example with sane defaults

**DoD**: /health returns 200 with "arangodb":"ok"; first KG query â‰¤1.5s P95; no secrets in logs

### **I2: Vector Lane Cold-Start Killer + Service Singletons** 
**Priority**: ğŸ”´ HIGH  
**Estimated Time**: 6-8 hours
**Status**: ğŸ“‹ TODO
**Owner**: TBD
**Maps to**: Prompt 2

#### **TODOs:**
- [ ] Introduce process-level singletons for embedder and vector store with thread/async safety
- [ ] Add preload() at startup: loads model, dummy embed, topK=1 search to warm Qdrant/Meilisearch/Chroma
- [ ] Ensure vector search path is async and guarded with asyncio.wait_for(..., 2.0)
- [ ] Add in-memory LRU embedding cache (keyed by normalized text) with TTL
- [ ] Log TFTI (time-to-first-inference) and TTS (time-to-search) separately

**DoD**: First vector query â‰¤2.0s P95 after warmup; subsequent queries â‰¤800ms median on CPU; one embedder/client per process

### **I3: Provider Router Enhancement**
**Priority**: ğŸ”´ HIGH  
**Estimated Time**: 4-6 hours  
**Status**: âœ… COMPLETED
**Owner**: AI Assistant
**Completion Date**: Current
**Maps to**: Prompt 3

#### **TODOs:**
- [x] Create single provider order registry (COMPLETED in B1/B2)
- [x] Feature flags: context length, streaming, cost tier (COMPLETED)
- [x] Add vision support, JSON mode flags to ModelConfig
- [x] Route by policy: free/local first, escalate on complexity, auto-fallback on errors/timeouts
- [x] Add router telemetry: provider chosen, reason, latency, tokens
- [x] Update .env.example with RPM/TPM budget caps

**DoD**: âœ… With no paid keys â†’ never blocks, completes via local/HF; âœ… with paid keys â†’ complex prompts route to GPT-4/Claude; âœ… router metrics at /metrics

**Key Accomplishments:**
- âœ… Enhanced ModelConfig with vision, JSON mode, and function calling support
- âœ… Implemented policy-based routing with free-first optimization
- âœ… Added circuit breaker pattern for provider reliability
- âœ… Comprehensive telemetry collection and metrics endpoint
- âœ… Budget management with RPM/TPM caps
- âœ… Auto-fallback with graceful degradation
- âœ… Enhanced router service with comprehensive capabilities

---

## ğŸ” **PHASE C: Zero-Budget Retrieval Aggregator + Citations/Fact-Check**

### **C1: Retrieval Aggregator - Free Sources at Scale + Dedupe**
**Priority**: ğŸ”´ HIGH  
**Estimated Time**: 6-8 hours
**Status**: âœ… COMPLETED
**Owner**: AI Assistant
**Completion Date**: Current
**Maps to**: Prompt 5

#### **TODOs:**
- [x] Add parallel fetchers for: Wikipedia, StackExchange, MDN, GitHub, OpenAlex, arXiv, YouTube (if key)
- [x] Implement polite rate limits and backoff per source
- [x] Normalize result schema; domain+title fuzzy dedupe
- [x] Rank by relevance Ã— credibility Ã— recency Ã— diversity  
- [x] Return topK diversified list plus raw pool for analysis
- [x] Cache hits with TTL; respect per-source API etiquette

**DoD**: âœ… Aggregated /search returns â‰¥6 unique, high-quality sources in <3s P95; âœ… duplicates collapse to one; âœ… source metadata includes provider, timestamp, score

**Key Accomplishments:**
- âœ… Created `RetrievalAggregator` service with parallel source fetching
- âœ… Implemented `RateLimiter` with exponential backoff per source
- âœ… Added intelligent deduplication using content hashing and title similarity
- âœ… Multi-factor ranking: relevance Ã— credibility Ã— recency Ã— diversity
- âœ… Comprehensive API endpoints: `/search`, `/status`, `/sources`, `/health`
- âœ… Zero-budget operation using free APIs (Wikipedia, StackExchange, MDN, GitHub)
- âœ… Caching with TTL and API etiquette respect
- âœ… Graceful degradation on source failures

### **C2: Citations Pass - Sentenceâ†’Source Alignment + Bibliography**
**Priority**: ğŸ”´ HIGH  
**Estimated Time**: 8-10 hours
**Status**: âœ… COMPLETED
**Owner**: AI Assistant
**Completion Date**: Current
**Maps to**: Prompt 4

#### **TODOs:**
- [x] Implement sentence-to-passage alignment (cosine similarity over sentence embeddings vs retrieved snippets)
- [x] Inject inline citation markers next to claims; attach confidence and disagreement flags  
- [x] Build bibliography block ordered by first occurrence; include title, URL, provider, timestamp
- [x] Frontend: render superscript markers, hover to preview snippet, side panel of sources
- [x] Add "Disagreeing sources" badge when conflicts detected

**DoD**: âœ… Every nontrivial claim bears a citation; âœ… "Disagreeing sources" badge appears when conflicts detected; âœ… copy/export to Markdown preserves markers and bibliography

**Key Accomplishments:**
- âœ… Created `CitationsService` with intelligent claim extraction
- âœ… Implemented `ClaimExtractor` using regex patterns for claim identification
- âœ… Added `SourceAligner` with TF-IDF vectorization and cosine similarity
- âœ… Created `DisagreementDetector` for conflict identification
- âœ… Built `Bibliography` class with Markdown and BibTeX export
- âœ… Comprehensive API endpoints: `/process`, `/export`, `/analyze`, `/health`
- âœ… Inline citation markers with confidence scoring
- âœ… Disagreement warnings and confidence flags
- âœ… Export support for academic and documentation formats

---

## ğŸ—„ï¸ **PHASE D: Index Fabric - Vector DB + Knowledge Graph**

### **D1: Index Fabric - Meilisearch + Qdrant + Chroma + KG**  
**Priority**: ğŸ”´ HIGH  
**Estimated Time**: 8-10 hours
**Status**: ğŸ“‹ TODO
**Owner**: TBD
**Maps to**: Prompt 6

#### **TODOs:**
- [ ] For each query, launch parallel enrichment lanes: Meilisearch keyword, Qdrant vector, Chroma local, KG entity expansion
- [ ] Merge via reciprocal rank fusion + source diversity cap
- [ ] Expose /status/indexers with sizes, freshness, last error
- [ ] Add backfill worker to continuously ingest popular queries & uploaded docs
- [ ] Standardize VectorStore interface with adapters for Qdrant + Chroma
- [ ] Prefer Qdrant for prod, allow Chroma for dev/ephemeral
- [ ] Enforce P95 budgets: embed â‰¤ 300ms, search â‰¤ 300ms, fuse â‰¤ 300ms after warm

**DoD**: Indexer lanes contribute results without delaying 3s budget; /status/indexers shows sizes & health; no red on normal flow

### **D2: Data Ingestion & Status Visibility**
**Priority**: ğŸŸ¡ MEDIUM  
**Estimated Time**: 6-8 hours
**Status**: ğŸ“‹ TODO
**Owner**: TBD

#### **TODOs:**
- [ ] Add `/upload` for PDF/MD/TXT with per-user/session collections
- [ ] Create `/status/vector` endpoint reporting collection sizes
- [ ] Implement background jobs to chunkâ†’embedâ†’upsert
- [ ] Add ingestion progress tracking

**DoD**: Uploadâ†’RAG works; status endpoints green; no blocking on ingest

---

## ğŸ“¡ **PHASE E: Streaming, Observability, Security (Prod-Grade)**

### **E1: SSE Streaming Done Right + Heartbeats + Trace IDs**
**Priority**: ğŸ”´ HIGH  
**Estimated Time**: 6-8 hours
**Status**: ğŸ“‹ TODO
**Owner**: TBD
**Maps to**: Prompt 7

#### **TODOs:**
- [ ] Emit proper SSE frames: event: token|heartbeat|final, id, retry, data
- [ ] Send heartbeat every 10s; cap total stream at 60s; close gracefully on client drop
- [ ] Propagate X-Trace-ID end-to-end; echo in each SSE event
- [ ] Client renders tokens as they arrive; attach citations live when known
- [ ] Production-grade streaming that survives proxies

**DoD**: Time-to-first-token < 1s on local; streams never stall silently; heartbeats visible in DevTools; trace ID appears in server logs and client SSE events

### **E2: Always-On Performance Suite - Enforce Budgets**
**Priority**: ğŸ”´ HIGH  
**Estimated Time**: 6-8 hours
**Status**: ğŸ“‹ TODO
**Owner**: TBD
**Maps to**: Prompt 8

#### **TODOs:**
- [ ] Finalize perf tests; make them CI-green
- [ ] Add per-lane budgets with asyncio.wait_for and graceful partial results
- [ ] Emit Prometheus metrics: p50/p95, errors, timeouts by lane/provider
- [ ] Fail CI if budgets regress by >10%
- [ ] Keep us within E2E â‰¤3s, Vector â‰¤2s, KG â‰¤1.5s

**DoD**: Test suite passes locally; CI gate toggled on; Grafana dashboards show budgets per lane

### **E3: Security & Legal Hardening**
**Priority**: ğŸ”´ HIGH  
**Estimated Time**: 6-8 hours
**Status**: ğŸ“‹ TODO
**Owner**: TBD
**Maps to**: Prompt 9

#### **TODOs:**
- [ ] Add CSP, HSTS, X-Frame-Options, Referrer-Policy; sanitize URLs and HTML; canonicalize host
- [ ] Rate limit: 60 rpm/IP with bursts; block known bad UA
- [ ] Footer disclosure: source attribution, limits, privacy
- [ ] Implement circuit breakers around all external calls with exponential backoff

**DoD**: Security headers verified in browser/network tab; abuse attempts return 429/400 with no crashes

### **E4: Audit & Provenance**
**Priority**: ğŸŸ¡ MEDIUM  
**Estimated Time**: 6-8 hours
**Status**: ğŸ“‹ TODO
**Owner**: TBD
**Maps to**: Prompt 10

#### **TODOs:**
- [ ] Log provenance record: query hash, sources used, provider route, citations, disagreements, timing
- [ ] Add GET /audit/{trace_id} to fetch last run's provenance
- [ ] Mask PII; rotate logs; configurable retention
- [ ] Trace every answer to sources, routes, and decisions

**DoD**: For any answer, you can pull an audit JSON that explains how we got it

---

## ğŸ¨ **PHASE F: Cosmic UI v2 (Next.js) with Astro-Inspired System**

### **F1: Cosmic UI v2 - Visual System Refresh**
**Priority**: ğŸŸ¡ MEDIUM  
**Estimated Time**: 10-12 hours
**Status**: ğŸ“‹ TODO
**Owner**: TBD
**Maps to**: Prompt 11

#### **TODOs:**
- [ ] Keep Next.js 15 (realtime, SSE, app router), upgrade look to premium cosmic design
- [ ] Install design token layer: spacing (8px scale), typography (2â€“3 sizes per breakpoint), color roles (primary/secondary/surface/overlay), elevation
- [ ] Add cosmic background system (GPU-cheap starfield, parallax nebula, no heavy canvas on mobile)
- [ ] Standardize card/list/table dimensions; consistent paddings; responsive grid
- [ ] Rework Search â†’ Answer â†’ Sources layout with sticky source panel, live citations, keyboard navigation
- [ ] Add theme toggle (light/dark cosmic) with identical layout metrics

**DoD**: Lighthouse Performance â‰¥90, Accessibility â‰¥95, Best Practices â‰¥95, SEO â‰¥90; UI looks premium across mobile/tablet/desktop with no jank

## ğŸ§¾ **PHASE G: Repository Hygiene & Documentation**

### **G1: Repo Hygiene - Unused/Duplicate Purge (Safe)**
**Priority**: ğŸŸ¡ MEDIUM  
**Estimated Time**: 4-6 hours
**Status**: âœ… PARTIALLY COMPLETED (Phase A)
**Owner**: AI Assistant
**Maps to**: Prompt 12

#### **TODOs:**
- [x] Scan all files; list unused/duplicate/unwired modules (COMPLETED in Phase A)
- [x] Extract reusable logic into current flow (COMPLETED)
- [x] Move questionable files to /archive/DATE/ (COMPLETED)
- [x] Produce code_garden/plan.json with delete/adopt/merge actions (COMPLETED)
- [ ] Final cleanup pass for any remaining duplicates identified

**DoD**: âœ… Working tree is smaller; âœ… tests still green; âœ… plan.json exists

### **G2: Docs Pack & README Index**
**Priority**: ğŸŸ¡ MEDIUM  
**Estimated Time**: 4-6 hours
**Status**: âœ… PARTIALLY COMPLETED (Phase A)
**Owner**: AI Assistant
**Maps to**: Prompt 13

#### **TODOs:**
- [x] Add Problem Definition, Market Analysis to docs/ (COMPLETED)
- [ ] Link them from README with "How this repo maps to the Solution" section
- [ ] Add "Quick Start" with Docker Compose + .env keys list
- [ ] Ensure in-repo indexing of core docs

**DoD**: Opening README gives clean docs index and one-command start

---

## ğŸ”— **PHASE H: End-to-End Integration & Testing**

### **H1: End-to-End Wire-Up Smoke Test**
**Priority**: ğŸ”´ HIGH  
**Estimated Time**: 8-10 hours
**Status**: ğŸ“‹ TODO
**Owner**: TBD
**Maps to**: Prompt 14

#### **TODOs:**
- [ ] Implement smoke test that calls /search with 3 sample queries
- [ ] Verify streaming events, citations present, sources resolve
- [ ] Assert budgets: E2E â‰¤3s, vector â‰¤2s (after warmup), KG â‰¤1.5s
- [ ] Open /search page headless; check UI tokens & citation hovers
- [ ] Validate whole chain: Query â†’ Retrieval â†’ Synthesis â†’ Citations â†’ Stream â†’ UI

**DoD**: All assertions pass locally; complete end-to-end validation

---

## ğŸ—„ï¸ **PHASE J: Datastore-Specific Optimizations**

### **J1: Meilisearch Tuning**
**Priority**: ğŸŸ¡ MEDIUM  
**Estimated Time**: 4-6 hours
**Status**: ğŸ“‹ TODO
**Owner**: TBD

#### **TODOs:**
- [ ] Tune index settings (stop-words, synonyms, searchable/sortable attrs) for domains (docs/code/QA)
- [ ] Batch updates with small chunk sizes; enable auto-refresh post-bulk
- [ ] Add `/status/search` reporting index sizes and last update

**DoD**: Fast keyword results; status endpoint green; ingestion metrics visible

### **J2: Qdrant Production Optimization**
**Priority**: ğŸ”´ HIGH  
**Estimated Time**: 4-6 hours
**Status**: ğŸ“‹ TODO
**Owner**: TBD

#### **TODOs:**
- [ ] Ensure single client, HTTP keep-alive, batch upserts, warmup at boot
- [ ] Verify collection params (vector size matches embedder)
- [ ] Add small cache for frequent queries

**DoD**: Warm vector queries < 1s; collection schema correct; status endpoint shows healthy collections

### **J3: Chroma Dev Environment**
**Priority**: ğŸŸ¢ LOW  
**Estimated Time**: 2-4 hours
**Status**: ğŸ“‹ TODO
**Owner**: TBD

#### **TODOs:**
- [ ] Use Chroma for ephemeral/dev with clear toggle; mirror VectorStore interface

**DoD**: Dev works without Qdrant; prod path remains Qdrant

---

## ğŸ“Š **SUCCESS METRICS TO WATCH**

### **Performance Metrics:**
- [ ] Time-to-first-token < 1s
- [ ] Response P95 < 3s after warm
- [ ] Cache hit > 30%
- [ ] Lane error < 5%

### **Quality Metrics:**
- [ ] Citation CTR > 15%
- [ ] Answer quality thumbs-up > 80%
- [ ] Stickiness (7-day return)
- [ ] Export rate > 10%

---

## ğŸ¯ **IMPLEMENTATION PRIORITY ORDER (Updated with 14 Prompts)**

### **ğŸ”´ CRITICAL PATH (Weeks 1-2) - Foundation + Infrastructure**: 
- âœ… A: Repository Hygiene (COMPLETED)
- âœ… B1: Centralize Provider Order + Free-First Fallback (COMPLETED)
- âœ… B2: Best-LLM-per-Role Policy (COMPLETED)
- I1: ArangoDB 401 + Graph Warm Path (Prompt 1)
- I2: Vector Lane Cold-Start Killer + Service Singletons (Prompt 2)
- I3: Provider Router Enhancement (Prompt 3) - PARTIALLY DONE
- B3: Always-On Multi-Lane Orchestration
- J2: Qdrant Production Optimization

### **ğŸ”´ HIGH PRIORITY (Weeks 2-3) - Core Features**: 
- C1: Retrieval Aggregator - Free Sources at Scale (Prompt 5)
- D1: Index Fabric - Meilisearch + Qdrant + Chroma + KG (Prompt 6)
- C2: Citations Pass - Sentenceâ†’Source Alignment (Prompt 4)
- E1: SSE Streaming Done Right + Heartbeats (Prompt 7)
- E2: Always-On Performance Suite (Prompt 8)
- E3: Security & Legal Hardening (Prompt 9)

### **ğŸŸ¡ MEDIUM PRIORITY (Weeks 3-4) - Polish & Quality**: 
- E4: Audit & Provenance (Prompt 10)
- F1: Cosmic UI v2 - Visual System Refresh (Prompt 11)
- H1: End-to-End Wire-Up Smoke Test (Prompt 14)
- D2: Data Ingestion & Status Visibility
- J1: Meilisearch Tuning
- G2: Docs Pack & README Index (Prompt 13)

### **ğŸŸ¢ LOW PRIORITY (Weeks 4-5) - Final Polish**: 
- G1: Final Repo Hygiene Pass (Prompt 12)
- J3: Chroma Dev Environment

---

## ğŸ§­ **RECOMMENDED RUN ORDER (Following Prompt Guidelines)**

**Phase 1**: Foundation + Infrastructure (Prompts 1, 2, 3)
- I1: ArangoDB auth fix
- I2: Vector cold-start killer  
- I3: Provider router enhancement

**Phase 2**: Retrieval + Index Fabric (Prompts 5, 6)
- C1: Free sources aggregator
- D1: Index fabric parallel lanes

**Phase 3**: Citations + Streaming UX (Prompts 4, 7)
- C2: Citation alignment
- E1: SSE streaming with heartbeats

**Phase 4**: Performance, Security, Audit (Prompts 8, 9, 10)
- E2: Performance budgets
- E3: Security hardening
- E4: Audit & provenance

**Phase 5**: UI + Documentation (Prompts 11, 12, 13, 14)
- F1: Cosmic UI v2
- G1-G2: Cleanup + docs
- H1: End-to-end smoke test

---
### after evry phase check that we are getting output using real env variables and  are not giving mock results ,
### NOTE: .env file exists in root dir with all variables, keys, values etc loaded. but cursor dont have permission to access it, just cross verify the key:value pairs key variables are being used in code to call the api keys or key:value pair from .env file.

#### if some api keys or values are wrong in .env file and the cursor cannot resolve it automatically, make a note to inform me to add in this implementation_tasks.md file itself at end.

### Always follow proper files and folders structure, keep microservices based structure. Always try to use latest stable tech, tech stacks and latest stable models, kg,vectordb, etc

## ğŸ“ˆ **PROJECT SUMMARY**

**Total Tasks**: 25 phases (integrated with 14 prompts)
**Completed**: 3 phases (A, B1, B2)
**In Progress**: 1 phase (I3 - partially done via B1/B2)
**Remaining**: 21 phases

**Estimated Total Time**: 130-170 hours 
**Estimated Completion**: 5-7 weeks with dedicated development
**MAANG Standard**: Production-ready with observability, security, and performance at scale

**Current Focus**: Ready for Phase I1 - ArangoDB 401 + Graph Warm Path
**Next Up**: Infrastructure Layer (I1, I2, I3) - Foundation for all other features

---

## ğŸ”„ **UPDATE HISTORY**

### **Latest Update - Integration with 14 MAANG-Level Prompts**
- **Date**: Current
- **Action**: Comprehensive integration of 14 production-grade prompts with existing implementation
- **Major Changes**:
  - âœ… **Phase A**: Repository hygiene and baseline (COMPLETED)
  - âœ… **Phase B1**: Centralized provider order system (COMPLETED)
  - âœ… **Phase B2**: Role-based model selection with dynamic complexity routing (COMPLETED)
  - ğŸ”„ **Restructured**: All remaining phases to align with MAANG/OpenAI/Perplexity standards
  - ğŸ†• **Added**: Infrastructure layer (I1-I3) for production foundations
  - ğŸ†• **Enhanced**: Citations, streaming, security, and performance with industry standards
  - ğŸ†• **Integrated**: 14 specific prompts mapped to implementation phases

### **Previous Updates**
- **Phase A**: Completed repository hygiene and baseline
- **Phase B1-B2**: Implemented comprehensive LLM provider system with role-based model selection
- **Status**: Ready for Infrastructure Layer (Phase I1) - ArangoDB auth + graph warm path

### **Key Accomplishments**
- âœ… Comprehensive codebase cleanup and organization
- âœ… Centralized LLM provider order system with free-first fallback
- âœ… Advanced role-based model selection (FAST/QUALITY/LONG/REASONING/TOOL)
- âœ… Dynamic model selection based on query complexity
- âœ… Industry-standard task organization aligned with MAANG practices

---

## ğŸ¯ **TECHNICAL EXCELLENCE STANDARDS**

**Why this matches MAANG/OpenAI/Perplexity standards:**

1. **Free-first with capability-aware routing** â†’ Best $/quality trade-off
2. **Non-blocking lanes + budgets** â†’ Responsiveness under failure  
3. **Evidence-first citations & provenance** â†’ Trust (core differentiator)
4. **Observability (metrics, trace IDs) + security headers** â†’ Production table stakes
5. **Astro-inspired UI without switching frameworks** â†’ Preserves SSE/React ecosystem strengths while hitting design bar
6. **Zero-budget optimization** â†’ Sustainable scaling with graceful premium upgrades

**Next milestone**: Infrastructure foundation (I1-I3) enabling all downstream features.

## To be super clear, hereâ€™s what still stands between you and â€œfixed allâ€:

## Must-close blockers (in order)

I1 ArangoDB auth + warm path â€“ unblocks KG lane and removes 401s.

I2 Vector cold-start + singletons â€“ drops vector P95 to â‰¤2s after warm.

B3 Always-on multi-lane orchestration â€“ ensures non-blocking returns when any lane is slow/down.

C1 Free-source aggregator â€“ makes zero-budget retrieval actually strong/fast.

C2 Citations pass â€“ evidence-first UX (inline markers + bibliography).

D1 Index fabric (Meilisearch+Qdrant+Chroma+KG) â€“ parallel enrichment without delaying the 3s budget.

E1/E2 Streaming + budgets â€“ reliable SSE (heartbeats) and enforced timeboxes.

J2 Qdrant tuning â€“ validates schema/params and locks in sub-second warm searches.

H1 End-to-end smoke test â€“ proves the full chain works under budgets.

## â€œDonâ€™t forgetâ€ items that often bite late

Router telemetry & budget caps exposed at /metrics.

Security headers + rate limiting + circuit breakers actually verified in browser and logs.

Secrets & config hygiene: single source of truth in .env + .env.example (no secrets in logs).

Backups & rollbacks for Qdrant/Meilisearch/Arango (snapshot/restore steps documented).

Scraper etiquette (per-source backoff, ToS awareness) in the free aggregator.

CI gates that fail on perf regressions (>10%), lint/test failures, or Lighthouse dips.

Accessibility pass (ARIA, contrast, keyboard nav) for the Cosmic UI v2.

How youâ€™ll know youâ€™ve truly â€œfixed allâ€

## Use these go/no-go gates (all must pass):

Gate 1 Â· Health: /health returns 200 with { arangodb:"ok", vector:"ok", meili:"ok" }.

Gate 2 Â· Performance: P95 E2E â‰¤3s, Vector lane â‰¤2s (warm), KG lane â‰¤1.5s; non-blocking with partials when a lane times out.

Gate 3 Â· Evidence: Every nontrivial sentence renders with [n] markers; bibliography preserved on export; â€œDisagreementâ€ badge appears on conflicts.

Gate 4 Â· Streaming: Time-to-first-token <1s locally; heartbeats every 10s; graceful close; X-Trace-ID visible in each SSE event.

Gate 5 Â· Security: CSP/HSTS/XFO/Referrer-Policy present; 60 rpm/IP rate limit; input sanitization verified; external calls wrapped with circuit breakers.

Gate 6 Â· Observability: Prometheus metrics show per-lane p50/p95 + errors/timeouts; /audit/{trace_id} returns provenance JSON.

Gate 7 Â· UI: Lighthouse Perf â‰¥90 / A11y â‰¥95 / BP â‰¥95 / SEO â‰¥90 across mobile/desktop; visual QA on searchâ†’answerâ†’sources flow.

Gate 8 Â· Docs: README indexes Problem Definition, Market Analysis, Solution; Quick Start (Docker Compose + .env); runbooks for ingest/backups.

## Bottom line

If you finish I1, I2, B3, C1, C2, D1, E1â€“E3, H1, J2 to spec and pass the gates above, youâ€™ll have closed all known issues and hit a MAANG/OpenAI/Perplexity-grade v1.

Anything beyond that (â€œbest-in-class at scaleâ€) becomes scaling work (multi-tenant isolation, quotas/billing, abuse detection, GPU/queueing, disaster recovery). Not required to declare â€œfixed all,â€ but worth scheduling after v1 passes the gates.