#!/usr/bin/env python3
"""
SarvanOM Universal Knowledge Platform - Real LLM Integration
Consolidated from all LLM client implementations with best features.

This is the UNIFIED LLM processor implementing:
- Multi-provider support (OpenAI, Anthropic, Ollama, HuggingFace)
- Intelligent fallback chains (local â†’ free â†’ paid)
- Query classification and dynamic routing
- Zero-budget optimization with free-tier APIs
- Real-time performance monitoring
- Comprehensive error handling and retry logic

Based on Sarvanom_blueprint.md specifications for multi-agent AI orchestration.
"""

# Load environment variables from .env file
try:
    from dotenv import load_dotenv
    load_dotenv()  # Load .env file if present
except ImportError:
    pass  # dotenv not installed, continue without it

import asyncio
import json
import time
import hashlib
import re
from typing import Dict, Any, List, Optional, Union
import os
from datetime import datetime
from dataclasses import dataclass, field
from enum import Enum

# LLM Integration with fallback imports
try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False

try:
    import aiohttp
    AIOHTTP_AVAILABLE = True
except ImportError:
    AIOHTTP_AVAILABLE = False

import requests

# Configuration from environment
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
HUGGINGFACE_API_KEY = os.getenv("HUGGINGFACE_API_KEY")
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
OLLAMA_TIMEOUT = int(os.getenv("OLLAMA_TIMEOUT", "30"))
OLLAMA_DEFAULT_MODEL = os.getenv("OLLAMA_DEFAULT_MODEL", "llama3")

# Zero-budget optimization flags
PRIORITIZE_FREE_MODELS = os.getenv("PRIORITIZE_FREE_MODELS", "true").lower() == "true"
USE_DYNAMIC_SELECTION = os.getenv("USE_DYNAMIC_SELECTION", "true").lower() == "true"
LLM_TIMEOUT_SECONDS = int(os.getenv("LLM_TIMEOUT_SECONDS", "15"))


class LLMProvider(str, Enum):
    """Supported LLM providers with zero-budget optimization."""
    OLLAMA = "ollama"      # Local models (free)
    HUGGINGFACE = "huggingface"  # Free tier API
    OPENAI = "openai"      # Paid (fallback)
    ANTHROPIC = "anthropic"  # Paid (fallback)
    MOCK = "mock"          # Testing fallback


class QueryComplexity(str, Enum):
    """Query complexity levels for dynamic routing."""
    SIMPLE_FACTUAL = "simple_factual"
    RESEARCH_SYNTHESIS = "research_synthesis"
    COMPLEX_REASONING = "complex_reasoning"


class LLMModel(str, Enum):
    """Comprehensive LLM model catalog from all implementations."""
    # OpenAI models (Latest 2025)
    OPENAI_GPT_4O = "gpt-4o"  # Latest multimodal model (Dec 2024+)
    OPENAI_GPT_4O_MINI = "gpt-4o-mini"  # Cost-efficient latest model
    OPENAI_GPT_4_TURBO = "gpt-4-turbo"  # Updated stable version
    OPENAI_GPT_4 = "gpt-4"
    OPENAI_O1_PREVIEW = "o1-preview"  # Latest reasoning model (Sept 2024+)
    OPENAI_O1_MINI = "o1-mini"  # Efficient reasoning model
    
    # Anthropic models (Latest 2025)
    ANTHROPIC_CLAUDE_3_5_SONNET = "claude-3-5-sonnet-20241022"  # Latest Claude 3.5 Sonnet (Oct 2024+)
    ANTHROPIC_CLAUDE_3_5_HAIKU = "claude-3-5-haiku-20241022"    # Latest Claude 3.5 Haiku (Oct 2024+) 
    ANTHROPIC_CLAUDE_3_OPUS = "claude-3-opus-20240229"          # Still latest Opus
    ANTHROPIC_CLAUDE_3_SONNET = "claude-3-sonnet-20240229"      # Legacy Sonnet
    ANTHROPIC_CLAUDE_3_HAIKU = "claude-3-haiku-20240307"        # Legacy Haiku
    
    # Local Ollama models (free)
    OLLAMA_LLAMA2 = "llama2"
    OLLAMA_LLAMA2_7B = "llama2:7b"
    OLLAMA_LLAMA2_13B = "llama2:13b"
    OLLAMA_LLAMA3 = "llama3"
    OLLAMA_LLAMA3_7B = "llama3:7b"
    OLLAMA_LLAMA3_13B = "llama3:13b"
    OLLAMA_MISTRAL = "mistral"
    OLLAMA_CODELLAMA = "codellama"
    
    # HuggingFace free models
    HF_DIALOGPT_MEDIUM = "microsoft/DialoGPT-medium"
    HF_DIALOGPT_LARGE = "microsoft/DialoGPT-large"
    HF_DISTILGPT2 = "distilgpt2"
    HF_GPT_NEO_125M = "EleutherAI/gpt-neo-125M"
    HF_LLAMA_2_7B = "meta-llama/Llama-2-7b-hf"
    HF_FALCON_7B = "tiiuae/falcon-7b"
    
    # Fallback
    MOCK_MODEL = "mock"


class EmbeddingModel(str, Enum):
    """Supported embedding models."""
    OPENAI_TEXT_EMBEDDING_3_SMALL = "text-embedding-3-small"
    OPENAI_TEXT_EMBEDDING_3_LARGE = "text-embedding-3-large"
    SENTENCE_TRANSFORMERS = "all-MiniLM-L6-v2"


@dataclass
class LLMRequest:
    """Unified LLM request structure."""
    prompt: str
    system_message: Optional[str] = None
    max_tokens: int = 1000
    temperature: float = 0.2
    complexity: QueryComplexity = QueryComplexity.RESEARCH_SYNTHESIS
    prefer_free: bool = True
    timeout: int = 15

class RealLLMProcessor:
    """Real LLM processing with multiple provider support."""
    
    def __init__(self):
        self.setup_clients()
    
    def setup_clients(self):
        """Setup LLM clients with zero-budget optimization."""
        self.clients = {}
        self.provider_health = {}
        
        # Setup clients based on availability and budget preference
        if OPENAI_AVAILABLE and OPENAI_API_KEY:
            try:
                # Try modern OpenAI client first
                self.openai_client = openai.OpenAI(api_key=OPENAI_API_KEY)
                self.clients[LLMProvider.OPENAI] = self.openai_client
                self.provider_health[LLMProvider.OPENAI] = True
            except Exception as e:
                print(f"OpenAI client setup failed: {e}")
                self.provider_health[LLMProvider.OPENAI] = False
            
        if ANTHROPIC_AVAILABLE and ANTHROPIC_API_KEY:
            self.anthropic_client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)
            self.clients[LLMProvider.ANTHROPIC] = self.anthropic_client
            self.provider_health[LLMProvider.ANTHROPIC] = True
        
        # Always available local/free options
        # HuggingFace as primary free provider (re-enabled with working models)
        self.provider_health[LLMProvider.OLLAMA] = True  # ENABLED - Ollama is running with deepseek-r1:8b
        self.provider_health[LLMProvider.HUGGINGFACE] = True  # ENABLED - Real API key working
        self.provider_health[LLMProvider.MOCK] = True
    
    def classify_query_complexity(self, query: str) -> QueryComplexity:
        """
        Advanced query complexity classification.
        
        Extracted and enhanced from shared/core/query_classifier.py and 
        shared/core/model_selector.py for better accuracy.
        """
        if not query or len(query.strip()) < 3:
            return QueryComplexity.SIMPLE_FACTUAL
        
        query_lower = query.lower()
        complexity_score = 0.0
        
        # Advanced pattern matching (extracted from query_classifier.py)
        
        # Complex reasoning indicators (high weight)
        complex_patterns = [
            r'\b(analyze|synthesize|evaluate|compare)\b.*\b(between|against|versus)\b',
            r'\bmulti[- ]?step\b',
            r'\b(comprehensive|thorough|detailed)\s+(analysis|review|evaluation)\b',
            r'\b(pros\s+and\s+cons|advantages\s+and\s+disadvantages)\b',
            r'\b(framework|methodology|approach|strategy)\b.*\b(develop|create|design)\b'
        ]
        
        for pattern in complex_patterns:
            if re.search(pattern, query_lower):
                complexity_score += 2.0
        
        # Research synthesis indicators (medium weight)
        research_patterns = [
            r'\b(research|study|findings|evidence|literature)\b',
            r'\b(recent|latest|current)\s+(research|developments|trends)\b',
            r'\b(academic|scientific|peer[- ]?reviewed)\b',
            r'\b(correlation|causation|relationship)\s+between\b',
            r'\b(survey|review|meta[- ]?analysis)\b'
        ]
        
        for pattern in research_patterns:
            if re.search(pattern, query_lower):
                complexity_score += 1.5
        
        # Analytical indicators (medium weight)
        analytical_patterns = [
            r'\bhow\s+(does|do|did)\b.*\bwork\b',
            r'\bwhy\s+(is|are|was|were|does|do)\b',
            r'\bexplain\s+the\s+(reason|process|mechanism)\b',
            r'\b(cause|effect|impact|consequence)\b.*\bof\b'
        ]
        
        for pattern in analytical_patterns:
            if re.search(pattern, query_lower):
                complexity_score += 1.0
        
        # Simple factual indicators (low weight)
        simple_patterns = [
            r'^\s*what\s+is\b',
            r'^\s*who\s+is\b', 
            r'^\s*when\s+(is|was|will)\b',
            r'^\s*where\s+(is|was|can)\b',
            r'^\s*(define|list|name)\b',
            r'^\s*how\s+many\b'
        ]
        
        for pattern in simple_patterns:
            if re.search(pattern, query_lower):
                complexity_score -= 0.5
        
        # Query length and structure analysis (from model_selector.py)
        words = query.split()
        word_count = len(words)
        
        if word_count > 50:
            complexity_score += 1.0
        elif word_count > 20:
            complexity_score += 0.5
        elif word_count < 5:
            complexity_score -= 0.5
        
        # Technical indicators
        technical_terms = ['api', 'algorithm', 'database', 'implementation', 'architecture', 'code', 'programming']
        if any(term in query_lower for term in technical_terms):
            complexity_score += 0.5
        
        # Classification based on score
        if complexity_score >= 2.0:
            return QueryComplexity.COMPLEX_REASONING
        elif complexity_score >= 0.5:
            return QueryComplexity.RESEARCH_SYNTHESIS
        else:
            return QueryComplexity.SIMPLE_FACTUAL
    
    def select_optimal_provider(self, complexity: QueryComplexity, prefer_free: bool = True) -> LLMProvider:
        """
        Enhanced provider selection with HuggingFace as primary free option.
        
        Priority: HuggingFace â†’ Ollama â†’ OpenAI â†’ Anthropic
        HuggingFace offers excellent free tier with diverse model selection.
        """
        if not USE_DYNAMIC_SELECTION:
            return self._get_fallback_provider()
        
        # Check which providers have working API keys
        has_openai = OPENAI_API_KEY and OPENAI_API_KEY.strip() and "your_" not in OPENAI_API_KEY
        has_anthropic = ANTHROPIC_API_KEY and ANTHROPIC_API_KEY.strip() and "your_" not in ANTHROPIC_API_KEY
        has_huggingface = HUGGINGFACE_API_KEY and HUGGINGFACE_API_KEY.strip() and "your_" not in HUGGINGFACE_API_KEY
        
        # PRIORITIZE WORKING API KEYS FOR FAST 5-SECOND RESPONSES
        if has_openai:
            print("ðŸš€ Selected OpenAI for fast response")
            return LLMProvider.OPENAI
        elif has_anthropic:
            print("ðŸš€ Selected Anthropic for fast response")
            return LLMProvider.ANTHROPIC
        elif has_huggingface:
            print("ðŸš€ Selected HuggingFace for free response")
            return LLMProvider.HUGGINGFACE
        
        # Zero-budget fallback: try free models only if no API keys
        elif prefer_free and PRIORITIZE_FREE_MODELS:
            if self.provider_health.get(LLMProvider.OLLAMA):
                print("âš ï¸ Using slow Ollama (no working API keys)")
                return LLMProvider.OLLAMA
        
        # For complex reasoning or when free options fail, escalate to paid
        if complexity == QueryComplexity.COMPLEX_REASONING:
            # For very complex queries, consider paid models
            if self.provider_health.get(LLMProvider.OPENAI):
                return LLMProvider.OPENAI
            elif self.provider_health.get(LLMProvider.ANTHROPIC):
                return LLMProvider.ANTHROPIC
        
        # Final fallback to any available provider
        return self._get_fallback_provider()
    
    def _get_fallback_provider(self) -> LLMProvider:
        """Get best available fallback provider - prioritize available providers."""
        
        # Check which providers have valid API keys (not placeholder values)
        has_huggingface = HUGGINGFACE_API_KEY and HUGGINGFACE_API_KEY.strip() and "your_" not in HUGGINGFACE_API_KEY
        has_openai = OPENAI_API_KEY and OPENAI_API_KEY.strip() and "your_" not in OPENAI_API_KEY  
        has_anthropic = ANTHROPIC_API_KEY and ANTHROPIC_API_KEY.strip() and "your_" not in ANTHROPIC_API_KEY
        has_ollama = True  # Ollama is always available locally
        
        print(f"ðŸ” Provider availability: HF={has_huggingface}, OpenAI={has_openai}, Anthropic={has_anthropic}, Ollama={has_ollama}")
        
        # PRIORITIZE WORKING API KEYS FOR FAST RESPONSES (5 seconds)
        if has_openai:
            print("ðŸš€ Using OpenAI (fast API - latest GPT-4o models)")
            return LLMProvider.OPENAI
        elif has_anthropic:
            print("ðŸš€ Using Anthropic (fast API - latest Claude 3.5)")
            return LLMProvider.ANTHROPIC
        elif has_huggingface:
            print("ðŸš€ Using HuggingFace (free API)")
            return LLMProvider.HUGGINGFACE
        elif has_ollama:
            print("âš ï¸ Using Ollama (slow local model - may timeout)")
            return LLMProvider.OLLAMA
        
        print("âŒ No providers available - using mock")
        return LLMProvider.MOCK
    
    async def search_with_ai(self, query: str, user_id: str = None, max_results: int = 10) -> Dict[str, Any]:
        """Process search query with AI enhancement using optimal provider selection."""
        start_time = time.time()
        
        # Classify query complexity for optimal provider selection
        complexity = self.classify_query_complexity(query)
        selected_provider = self.select_optimal_provider(complexity, prefer_free=True)
        
        # Enhanced search processing with AI
        search_prompt = f"""
        You are an expert research assistant for the SarvanOM Universal Knowledge Platform.
        
        User Query: "{query}"
        Query Complexity: {complexity.value}
        
        Provide a comprehensive search analysis that includes:
        1. Query intent classification (factual, research, comparative, etc.)
        2. Key search terms and concepts to explore
        3. Suggested information sources and databases
        4. Potential follow-up questions for deeper research
        5. Complexity assessment (simple, medium, complex)
        
        Respond in JSON format with structured data.
        """
        
        # Create LLM request with optimal settings
        llm_request = LLMRequest(
            prompt=search_prompt,
            complexity=complexity,
            max_tokens=800,
            timeout=LLM_TIMEOUT_SECONDS
        )
        
        ai_analysis = await self._call_llm_with_provider(llm_request, selected_provider)
        
        # Generate search results with AI-enhanced metadata
        results = self._generate_search_results(query, ai_analysis)
        
        processing_time = time.time() - start_time
        
        return {
            "query": query,
            "user_id": user_id,
            "max_results": max_results,
            "processing_time_ms": int(processing_time * 1000),
            "results": results,
            "total_results": len(results),
            "ai_analysis": ai_analysis,
            "query_classification": self._extract_classification(ai_analysis),
            "complexity_score": complexity.value,
            "selected_provider": selected_provider.value,
            "zero_budget_mode": PRIORITIZE_FREE_MODELS,
            "timestamp": datetime.now().isoformat(),
            "search_strategy": "ai_enhanced_hybrid"
        }
    
    async def fact_check_with_ai(self, claim: str, sources: List[Dict[str, Any]] = None, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """Perform AI-powered fact-checking."""
        if sources is None:
            sources = []
        
        start_time = time.time()
        
        # Create sources text
        sources_text = "\n".join([f"- {source.get('title', 'Source')}: {source.get('content', source.get('snippet', ''))}" for source in sources[:5]])
        
        fact_check_prompt = f"""
        You are an expert fact-checker for the SarvanOM Universal Knowledge Platform.
        
        Claim to verify: "{claim}"
        Context: {json.dumps(context) if context else "General fact-checking"}
        
        Reference Sources:
        {sources_text}
        
        Perform a comprehensive fact-check analysis:
        1. Assess the verifiability of the claim
        2. Identify key factual elements that can be verified
        3. Determine the claim's accuracy based on available information
        4. Provide confidence score (0.0 to 1.0)
        5. Suggest verification sources and methods
        6. Flag any potential biases or limitations
        
        Respond in JSON format with structured verification data.
        """
        
        ai_verification = await self._call_llm(fact_check_prompt, max_tokens=1000)
        
        processing_time = time.time() - start_time
        
        # Extract verification details
        verification_result = self._parse_fact_check_result(ai_verification)
        
        return {
            "claim": claim,
            "context": context,
            "verification_status": verification_result.get("status", "analyzed"),
            "confidence_score": verification_result.get("confidence", 0.8),
            "consensus_score": verification_result.get("consensus", 0.75),
            "evidence_sources": verification_result.get("sources", []),
            "verification_details": ai_verification,
            "processing_time_ms": int(processing_time * 1000),
            "timestamp": datetime.now().isoformat(),
            "expert_analysis": verification_result.get("analysis", "AI-powered verification completed"),
            "limitations": verification_result.get("limitations", []),
            "follow_up_needed": verification_result.get("follow_up", False)
        }
    
    async def synthesize_with_ai(self, content: str = None, query: str = None, style: str = "default", 
                               sources: List[Any] = None, target_audience: str = None) -> Dict[str, Any]:
        """AI-powered content synthesis and analysis."""
        if sources is None:
            sources = []
        
        # Use query as content if content is not provided
        if content is None and query:
            content = query
        elif content is None:
            content = "General synthesis request"
            
        start_time = time.time()
        
        synthesis_prompt = f"""
        You are an expert knowledge synthesizer for the SarvanOM Universal Knowledge Platform.
        
        Content to synthesize: "{content}"
        Query context: {query or "General synthesis"}
        Target audience: {target_audience or "General audience"}
        Style: {style}
        Sources to consider: {sources or ["general knowledge"]}
        
        Create a comprehensive synthesis that includes:
        1. Executive summary of key points
        2. Detailed analysis and insights
        3. Connections between different concepts
        4. Implications and conclusions
        5. Recommendations for further exploration
        6. Source reliability assessment
        
        Adapt the tone and complexity for the target audience.
        Respond in JSON format with structured synthesis data.
        """
        
        ai_synthesis = await self._call_llm(synthesis_prompt, max_tokens=1500)
        
        # Handle case where LLM call returns None - use enhanced fallback
        if ai_synthesis is None:
            ai_synthesis = await self._generate_fallback_response(synthesis_prompt)
        
        processing_time = time.time() - start_time
        
        # Parse synthesis results
        synthesis_result = self._parse_synthesis_result(ai_synthesis)
        
        return {
            "content": content,
            "query": query,
            "style": style,
            "target_audience": target_audience,
            "sources": sources or [],
            "synthesis": synthesis_result.get("synthesis", synthesis_result.get("response", ai_synthesis)),
            "executive_summary": synthesis_result.get("summary", synthesis_result.get("analysis", "AI synthesis completed")),
            "key_insights": synthesis_result.get("insights", synthesis_result.get("key_terms", [])),
            "recommendations": synthesis_result.get("recommendations", synthesis_result.get("suggested_sources", [])),
            "confidence_score": synthesis_result.get("confidence", 0.85),
            "processing_time_ms": int(processing_time * 1000),
            "timestamp": datetime.now().isoformat(),
            "synthesis_method": "multi_agent_ai",
            "quality_score": synthesis_result.get("quality", 0.8)
        }
    
    async def _call_llm_with_provider(self, request: LLMRequest, provider: LLMProvider) -> str:
        """Call specific LLM provider with intelligent fallback."""
        try:
            if provider == LLMProvider.OLLAMA:
                return await self._call_ollama(request.prompt, request.max_tokens, request.temperature)
            elif provider == LLMProvider.HUGGINGFACE:
                return await self._call_huggingface(request.prompt, request.max_tokens, request.temperature)
            elif provider == LLMProvider.OPENAI and LLMProvider.OPENAI in self.clients:
                return await self._call_openai(request.prompt, request.max_tokens, request.temperature)
            elif provider == LLMProvider.ANTHROPIC and LLMProvider.ANTHROPIC in self.clients:
                return await self._call_anthropic(request.prompt, request.max_tokens, request.temperature)
            else:
                # Fallback to next best provider
                fallback_provider = self._get_fallback_provider()
                if fallback_provider != provider and fallback_provider != LLMProvider.MOCK:
                    return await self._call_llm_with_provider(request, fallback_provider)
                else:
                    return self._generate_fallback_response(request.prompt)
        except Exception as e:
            print(f"{provider.value} call failed: {e}")
            # Try fallback provider
            fallback_provider = self._get_fallback_provider()
            if fallback_provider != provider and fallback_provider != LLMProvider.MOCK:
                return await self._call_llm_with_provider(request, fallback_provider)
            else:
                return self._generate_fallback_response(request.prompt)

    async def _call_llm(self, prompt: str, max_tokens: int = 1000, temperature: float = 0.2) -> str:
        """Legacy method - call LLM with automatic provider selection."""
        complexity = self.classify_query_complexity(prompt)
        provider = self.select_optimal_provider(complexity)
        request = LLMRequest(prompt=prompt, max_tokens=max_tokens, temperature=temperature)
        return await self._call_llm_with_provider(request, provider)
    
    async def _call_openai(self, prompt: str, max_tokens: int, temperature: float) -> str:
        """Call OpenAI API."""
        try:
            if not OPENAI_AVAILABLE or not OPENAI_API_KEY:
                return None
                
            # Select optimal accessible model based on task complexity (2025 models)
            if len(prompt) > 1000 or "reasoning" in prompt.lower() or "complex" in prompt.lower():
                model = "gpt-4o"  # Latest multimodal model for complex tasks
            elif max_tokens > 1000:
                model = "gpt-4o"  # Latest multimodal model for long responses
            else:
                model = "gpt-4o-mini"  # Cost-efficient latest model for standard tasks
            
            # Use the pre-configured OpenAI client with latest models
            response = await asyncio.to_thread(
                self.openai_client.chat.completions.create,
                model=model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=max_tokens,
                temperature=temperature,
                timeout=LLM_TIMEOUT_SECONDS
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"OpenAI API error: {e}")
            return None
    
    async def _call_anthropic(self, prompt: str, max_tokens: int, temperature: float) -> str:
        """Call Anthropic API."""
        try:
            # Select optimal latest Claude model based on task complexity (2025 models)
            if len(prompt) > 1000 or max_tokens > 1000:
                model = "claude-3-5-sonnet-20241022"  # Latest Claude 3.5 Sonnet for complex tasks
            else:
                model = "claude-3-5-haiku-20241022"   # Latest Claude 3.5 Haiku for efficient tasks
            
            message = await asyncio.to_thread(
                self.anthropic_client.messages.create,
                model=model,
                max_tokens=max_tokens,
                temperature=temperature,
                messages=[{"role": "user", "content": prompt}]
            )
            return message.content[0].text
        except Exception as e:
            print(f"Anthropic API error: {e}")
            return None
    
    def _select_ollama_model(self, prompt: str, max_tokens: int) -> str:
        """Select optimal Ollama model based on query characteristics."""
        prompt_lower = prompt.lower()
        
        # Use available DeepSeek R1 model for all tasks (it's currently the only available model)
        # DeepSeek R1 is a powerful reasoning model that can handle various tasks well
        return "deepseek-r1:8b"

    async def _call_ollama(self, prompt: str, max_tokens: int, temperature: float) -> str:
        """Enhanced Ollama integration with intelligent model selection."""
        try:
            # Select optimal model based on query characteristics
            selected_model = self._select_ollama_model(prompt, max_tokens)
            
            if not AIOHTTP_AVAILABLE:
                # Fallback to requests with enhanced error handling
                response = requests.post(
                    f"{OLLAMA_BASE_URL}/api/generate",
                    json={
                        "model": selected_model,
                        "prompt": prompt,
                        "options": {
                            "temperature": temperature, 
                            "num_predict": max_tokens,
                            "top_p": 0.9,
                            "top_k": 40
                        }
                    },
                    timeout=OLLAMA_TIMEOUT
                )
                if response.status_code == 200:
                    result = response.json()
                    return result.get("response", "")
                else:
                    # Try fallback model if primary fails
                    if selected_model != "llama3":
                        response = requests.post(
                            f"{OLLAMA_BASE_URL}/api/generate",
                            json={
                                "model": "llama3",
                                "prompt": prompt,
                                "options": {"temperature": temperature, "num_predict": max_tokens}
                            },
                            timeout=OLLAMA_TIMEOUT
                        )
                        if response.status_code == 200:
                            return response.json().get("response", "")
                return None
            
            async with aiohttp.ClientSession() as session:
                # Simple, fast payload for quick responses (5 seconds target)
                payload = {
                    "model": selected_model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": temperature,
                        "num_predict": min(max_tokens, 150),  # Limit tokens for faster response
                        "stop": ["\n\n", "Human:", "Assistant:"],  # Stop tokens for cleaner responses
                        "num_ctx": 1024  # Smaller context for faster processing
                    }
                }
                
                # Fast timeout for quick responses (5-10 seconds max)
                timeout = aiohttp.ClientTimeout(total=15)
                
                async with session.post(
                    f"{OLLAMA_BASE_URL}/api/generate",
                    json=payload,
                    timeout=timeout
                ) as response:
                    if response.status == 200:
                        # For non-streaming, expect simple JSON response
                        try:
                            result = await response.json()
                            response_text = result.get("response", "")
                            if response_text:
                                return response_text
                            else:
                                print(f"Ollama empty response: {result}")
                                return None
                        except Exception as parse_error:
                            print(f"Ollama JSON parse error: {parse_error}")
                            # Try reading as text
                            text_response = await response.text()
                            print(f"Ollama raw response: {text_response[:200]}...")
                            return None
                    else:
                        error_text = await response.text()
                        print(f"Ollama API error {response.status}: {error_text}")
                        return None
            return None
        except Exception as e:
            print(f"Ollama API error: {e}")
            return None
    
    async def _call_huggingface(self, prompt: str, max_tokens: int, temperature: float) -> str:
        """
        Call HuggingFace Inference API with free tier models.
        
        Optimized for free-tier models (gpt2, distilgpt2) with proper error handling
        and fallback logic for zero-budget optimization.
        """
        # HuggingFace works with free models even without API key for many cases
        # But API key provides better rate limits and reliability
            
        try:
            headers = {"Authorization": f"Bearer {HUGGINGFACE_API_KEY}"}
            
            # Enhanced model selection based on query characteristics
            model_name = self._select_huggingface_model(prompt, max_tokens)
            api_url = f"https://api-inference.huggingface.co/models/{model_name}"
            
            # Enhanced payload optimized for different model types
            if "bert" in model_name.lower():
                # BERT-based models (Q&A, classification)
                payload = {
                    "inputs": {
                        "question": "What is the answer?",
                        "context": prompt
                    }
                } if "squad" in model_name else {
                    "inputs": prompt
                }
            elif "bart" in model_name.lower():
                # BART summarization models
                payload = {
                    "inputs": prompt,
                    "parameters": {
                        "max_length": min(max_tokens, 200),
                        "min_length": 10,
                        "do_sample": False
                    }
                }
            else:
                # GPT-2, DialoGPT and other generation models
                payload = {
                    "inputs": prompt,
                    "parameters": {
                        "max_new_tokens": min(max_tokens, 200),  # Conservative for free tier
                        "temperature": max(temperature, 0.1),  # Ensure positive temperature
                        "do_sample": temperature > 0.1,
                        "top_p": 0.9,
                        "repetition_penalty": 1.1,
                        "return_full_text": False
                    },
                    "options": {
                        "wait_for_model": True,
                        "use_cache": True
                    }
                }
            
            if AIOHTTP_AVAILABLE:
                async with aiohttp.ClientSession() as session:
                    async with session.post(
                        api_url,
                        headers=headers,
                        json=payload,
                        timeout=aiohttp.ClientTimeout(total=LLM_TIMEOUT_SECONDS)
                    ) as response:
                        if response.status == 200:
                            result = await response.json()
                            return self._parse_huggingface_response(result, model_name)
                        elif response.status == 503:
                            # Model loading, try fallback model
                            return await self._call_huggingface_fallback(prompt, max_tokens, temperature)
                        else:
                            # Log error but don't print to console in production
                            error_text = await response.text()
                            print(f"HuggingFace API Error: {response.status} - Model: {model_name}")
                            # Try fallback on any error
                            return await self._call_huggingface_fallback(prompt, max_tokens, temperature)
            else:
                response = requests.post(
                    api_url,
                    headers=headers,
                    json=payload,
                    timeout=LLM_TIMEOUT_SECONDS
                )
                if response.status_code == 200:
                    result = response.json()
                    return self._parse_huggingface_response(result, model_name)
                elif response.status_code == 503:
                    # Model loading, try fallback
                    return await self._call_huggingface_fallback(prompt, max_tokens, temperature)
            
            return None
        except Exception as e:
            print(f"HuggingFace API error: {e}")
            return None
    
    def _select_huggingface_model(self, prompt: str, max_tokens: int) -> str:
        """
        Advanced HuggingFace model selection leveraging all advantages:
        - Specialized models for different domains
        - Free tier optimization
        - Task-specific model routing
        """
        prompt_lower = prompt.lower()
        
        # SPECIALIZED MODELS FOR DIFFERENT DOMAINS
        
        # Use only models that are confirmed available on HuggingFace free tier
        
        # Text Generation/Creative/Conversational - use GPT models (most reliable)
        if any(term in prompt_lower for term in ['generate', 'create', 'write', 'story', 'creative', 'compose', 'explain', 'describe', 'tell me', 'how does', 'what is']):
            return "gpt2"  # Most reliable text generation model
        
        # Question Answering - use available Q&A models
        if any(term in prompt_lower for term in ['?', 'question', 'answer', 'who', 'what', 'when', 'where', 'why', 'how']):
            return "gpt2"  # Use GPT-2 for Q&A as well
        
        # Summarization tasks - use simple models that work
        if any(term in prompt_lower for term in ['summarize', 'summary', 'key points', 'overview', 'brief']):
            return "gpt2"  # GPT-2 can handle summarization
        
        # Scientific/Research/Code/Medical/Financial/Legal content - use reliable models
        if any(term in prompt_lower for term in ['research', 'study', 'scientific', 'analysis', 'code', 'programming', 'medical', 'healthcare', 'financial', 'business', 'legal', 'law']):
            return "gpt2"  # Fallback to reliable GPT-2 for specialized content
        
        # Complex reasoning - use larger models
        if len(prompt) > 200 or max_tokens > 300:
            return "microsoft/DialoGPT-medium"  # Larger model for complex tasks
        
        # Default: Fast, reliable model for general queries
        return "distilgpt2"  # Fast and efficient default
    
    def _parse_huggingface_response(self, result: Any, model_name: str) -> str:
        """Advanced HuggingFace response parsing for different model types and tasks."""
        try:
            if isinstance(result, list) and len(result) > 0:
                first_result = result[0]
                
                # Text Generation models (GPT-2, DialoGPT)
                if "generated_text" in first_result:
                    return first_result["generated_text"]
                
                # Translation models
                if "translation_text" in first_result:
                    return first_result["translation_text"]
                
                # Summarization models (BART)
                if "summary_text" in first_result:
                    return first_result["summary_text"]
                
                # Question Answering models (BERT, DistilBERT)
                if "answer" in first_result:
                    return first_result["answer"]
                
                # Sentiment Analysis models
                if "label" in first_result and "score" in first_result:
                    label = first_result["label"]
                    score = first_result["score"]
                    return f"Sentiment: {label} (confidence: {score:.2f})"
                
                # Classification models
                if "labels" in first_result:
                    labels = first_result["labels"]
                    scores = first_result.get("scores", [])
                    if labels and scores:
                        return f"Classification: {labels[0]} (confidence: {scores[0]:.2f})"
                
            elif isinstance(result, dict):
                # Direct dictionary responses
                if "generated_text" in result:
                    return result["generated_text"]
                if "answer" in result:
                    return result["answer"]
                if "summary_text" in result:
                    return result["summary_text"]
            
            # Fallback - convert to string
            return str(result) if result else "No response generated"
                
        except Exception as e:
            print(f"Error parsing HuggingFace response for {model_name}: {e}")
            return f"Response parsing failed for {model_name}"
    
    async def _call_huggingface_fallback(self, prompt: str, max_tokens: int, temperature: float) -> str:
        """Fallback to verified working HuggingFace models when primary fails."""
        fallback_models = ["gpt2", "distilgpt2"]  # Only use verified working free models
        
        for model in fallback_models:
            try:
                headers = {"Authorization": f"Bearer {HUGGINGFACE_API_KEY}"}
                api_url = f"https://api-inference.huggingface.co/models/{model}"
                
                payload = {
                    "inputs": prompt[:500],  # Truncate for faster models
                    "parameters": {
                        "max_new_tokens": min(max_tokens, 256),
                        "temperature": temperature,
                        "return_full_text": False
                    }
                }
                
                if AIOHTTP_AVAILABLE:
                    async with aiohttp.ClientSession() as session:
                        async with session.post(
                            api_url,
                            headers=headers,
                            json=payload,
                            timeout=aiohttp.ClientTimeout(total=10)  # Shorter timeout for fallback
                        ) as response:
                            if response.status == 200:
                                result = await response.json()
                                parsed = self._parse_huggingface_response(result, model)
                                if parsed:
                                    return parsed
                else:
                    response = requests.post(
                        api_url,
                        headers=headers,
                        json=payload,
                        timeout=10
                    )
                    if response.status_code == 200:
                        result = response.json()
                        parsed = self._parse_huggingface_response(result, model)
                        if parsed:
                            return parsed
            except Exception as e:
                print(f"HuggingFace fallback error with {model}: {e}")
                continue
        
        return None
    
    async def _generate_fallback_response(self, prompt: str) -> str:
        """
        Enhanced fallback using HuggingFace free models instead of mock responses.
        Tries multiple HuggingFace models as fallback before giving up.
        """
        fallback_models = [
            "distilgpt2",           # Fast and reliable
            "gpt2",                 # Robust text generation
            "microsoft/DialoGPT-small"  # Conversational
        ]
        
        for model in fallback_models:
            try:
                # Try each fallback model
                if HUGGINGFACE_API_KEY:
                    headers = {"Authorization": f"Bearer {HUGGINGFACE_API_KEY}"}
                    api_url = f"https://api-inference.huggingface.co/models/{model}"
                    
                    # Simplified payload for fallback
                    payload = {
                        "inputs": prompt[:200],  # Truncate for fallback
                        "parameters": {
                            "max_new_tokens": 100,
                            "temperature": 0.7,
                            "return_full_text": False
                        }
                    }
                    
                    if AIOHTTP_AVAILABLE:
                        async with aiohttp.ClientSession() as session:
                            async with session.post(
                                api_url,
                                headers=headers,
                                json=payload,
                                timeout=aiohttp.ClientTimeout(total=10)
                            ) as response:
                                if response.status == 200:
                                    result = await response.json()
                                    parsed = self._parse_huggingface_response(result, model)
                                    if parsed and parsed != "No response generated":
                                        return parsed
                    else:
                        # Fallback to requests
                        response = requests.post(
                            api_url,
                            headers=headers,
                            json=payload,
                            timeout=10
                        )
                        if response.status_code == 200:
                            result = response.json()
                            parsed = self._parse_huggingface_response(result, model)
                            if parsed and parsed != "No response generated":
                                return parsed
                                
            except Exception as e:
                print(f"Fallback model {model} failed: {e}")
                continue
        
        # Final fallback - return meaningful response without mock data
        return f"Unable to process the query '{prompt[:50]}...' at this time. Please try again later or rephrase your question."
    
    def _generate_search_results(self, query: str, ai_analysis: str) -> List[Dict[str, Any]]:
        """Generate realistic search results based on query and AI analysis."""
        # This would normally query actual databases, but for demo we'll create structured results
        results = []
        
        # Extract key terms for result generation
        key_terms = query.split()[:3]  # Simple extraction
        
        for i in range(min(5, len(key_terms) + 2)):
            result = {
                "id": f"result_{i+1}",
                "title": f"Research on {' '.join(key_terms[:2])} - Source {i+1}",
                "url": f"https://academic-source-{i+1}.com/research",
                "snippet": f"Comprehensive analysis of {query[:50]}... discussing key findings and implications.",
                "relevance_score": 0.9 - (i * 0.1),
                "source_type": ["academic", "industry", "government", "expert"][i % 4],
                "publication_date": "2024-01-15",
                "author": f"Research Team {i+1}",
                "citations": 150 - (i * 20)
            }
            results.append(result)
        
        return results
    
    def _extract_classification(self, ai_analysis: str) -> str:
        """Extract query classification from AI analysis."""
        try:
            if isinstance(ai_analysis, str) and ai_analysis.startswith('{'):
                data = json.loads(ai_analysis)
                return data.get("intent", "research_query")
        except:
            pass
        return "research_query"
    
    def _calculate_complexity(self, query: str) -> float:
        """Calculate query complexity score."""
        complexity_indicators = ["analyze", "compare", "synthesize", "evaluate", "comprehensive"]
        score = 0.5  # base score
        
        for indicator in complexity_indicators:
            if indicator in query.lower():
                score += 0.1
        
        # Length factor
        if len(query.split()) > 10:
            score += 0.2
        
        return min(score, 1.0)
    
    def _parse_fact_check_result(self, ai_verification: str) -> Dict[str, Any]:
        """Parse AI fact-check result."""
        try:
            if isinstance(ai_verification, str) and ai_verification.startswith('{'):
                return json.loads(ai_verification)
        except:
            pass
        
        return {
            "status": "analyzed",
            "confidence": 0.8,
            "consensus": 0.75,
            "sources": ["ai_analysis"],
            "analysis": ai_verification[:200] + "..." if len(ai_verification) > 200 else ai_verification,
            "limitations": ["ai_generated"],
            "follow_up": False
        }
    
    def _parse_synthesis_result(self, ai_synthesis: str) -> Dict[str, Any]:
        """Parse AI synthesis result."""
        try:
            if isinstance(ai_synthesis, str) and ai_synthesis.startswith('{'):
                return json.loads(ai_synthesis)
        except:
            pass
        
        # Ensure ai_synthesis is not None
        if ai_synthesis is None:
            ai_synthesis = "Synthesis result unavailable"
            
        return {
            "synthesis": ai_synthesis,
            "summary": ai_synthesis[:200] + "..." if len(ai_synthesis) > 200 else ai_synthesis,
            "insights": ["ai_generated_insights"],
            "recommendations": ["further_research"],
            "confidence": 0.85,
            "quality": 0.8
        }

    async def generate_citations(self, content: str, sources: List[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Generate proper citations for content based on sources."""
        if sources is None:
            sources = []
            
        try:
            # Create citation prompt
            sources_text = "\n".join([
                f"{i+1}. {source.get('title', 'Unknown Title')} - {source.get('url', 'No URL')} ({source.get('type', 'web')})"
                for i, source in enumerate(sources[:10])
            ])
            
            prompt = f"""
Generate proper citations for the following content:

Content: {content}

Available Sources:
{sources_text}

Please provide:
1. In-text citations in the content where appropriate
2. A formatted bibliography/references section
3. Citation style: APA format

Citations:"""

            llm_response = await self._call_optimal_llm(prompt, max_tokens=500, temperature=0.1)
            
            return {
                "success": True,
                "citations": llm_response,
                "provider": self.last_used_provider.value if self.last_used_provider else "unknown",
                "sources_cited": len(sources or []),
                "citation_style": "APA"
            }
        except Exception as e:
            print(f"Citation generation error: {e}")
            return {
                "success": False,
                "citations": f"Basic citations: {', '.join([s.get('title', 'Source') for s in (sources or [])[:3]])}",
                "error": str(e),
                "sources_cited": len(sources or [])
            }

    async def review_content(self, content: str, sources: List[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Review content for quality, accuracy, and completeness."""
        if sources is None:
            sources = []
            
        try:
            # Create review prompt
            sources_text = "\n".join([f"- {source.get('title', 'Source')}: {source.get('content', source.get('snippet', ''))}" for source in sources[:3]])
            
            prompt = f"""
Review the following content for quality, accuracy, and completeness:

Content: {content}

Reference Sources:
{sources_text}

Please provide a review that includes:
1. Overall quality score (0-10)
2. Accuracy assessment
3. Completeness evaluation
4. Specific suggestions for improvement
5. Factual corrections if needed

Review:"""

            llm_response = await self._call_optimal_llm(prompt, max_tokens=600, temperature=0.2)
            
            # Extract quality score if possible
            quality_score = 0.8  # Default
            if "score:" in llm_response.lower():
                try:
                    score_text = llm_response.lower().split("score:")[1].split()[0]
                    quality_score = float(score_text.replace("/10", "")) / 10
                except:
                    pass
            
            return {
                "success": True,
                "review": llm_response,
                "quality_score": quality_score,
                "provider": self.last_used_provider.value if self.last_used_provider else "unknown",
                "sources_reviewed": len(sources or [])
            }
        except Exception as e:
            print(f"Content review error: {e}")
            return {
                "success": False,
                "review": f"Content appears to be about: {content[:100]}... Review unavailable.",
                "quality_score": 0.5,
                "error": str(e),
                "sources_reviewed": len(sources or [])
            }

# Global processor instance
real_llm_processor = RealLLMProcessor()
