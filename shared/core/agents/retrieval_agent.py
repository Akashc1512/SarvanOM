"""
Retrieval Agent Implementation
"""

import asyncio
import logging
import time
import os
import re
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from enum import Enum

from dotenv import load_dotenv

# Load environment variables
load_dotenv()

from shared.core.agents.base_agent import (
    BaseAgent,
    AgentType,
    AgentMessage,
    AgentResult,
    QueryContext,
)
from shared.core.agents.data_models import RetrievalResult, DocumentModel
from shared.core.agents.agent_utilities import (
    AgentTaskProcessor,
    ResponseFormatter,
    time_agent_function,
)
from shared.core.agents.validation_utilities import CommonValidators

# Configure logging
from shared.core.unified_logging import get_logger

logger = get_logger(__name__)

# Environment configuration
DEFAULT_TOKEN_BUDGET = int(os.getenv("DEFAULT_TOKEN_BUDGET", "1000"))


@dataclass
class Document:
    """Represents a retrieved document/chunk"""

    content: str
    score: float
    source: str
    metadata: Dict[str, Any] = field(default_factory=dict)
    doc_id: str = ""
    chunk_id: Optional[str] = None
    timestamp: Optional[str] = None

    def to_dict(self) -> Dict[str, Any]:
        return {
            "content": self.content,
            "score": self.score,
            "source": self.source,
            "metadata": self.metadata,
            "doc_id": self.doc_id,
            "chunk_id": self.chunk_id,
            "timestamp": self.timestamp,
        }


@dataclass
class KnowledgeTriple:
    """Represents a fact from knowledge graph"""

    subject: str
    predicate: str
    object: str
    confidence: float = 1.0
    source: str = "knowledge_graph"
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class SearchResult:
    """Unified search result format"""

    documents: List[Document]
    search_type: str
    query_time_ms: int
    total_hits: int
    metadata: Dict[str, Any] = field(default_factory=dict)


class EntityExtractor:
    """Entity extraction utilities."""

    def __init__(self):
        """Initialize entity extractor."""
        self.logger = get_logger(f"{__name__}.entity_extractor")

    async def extract_entities(self, query: str) -> List[Dict[str, Any]]:
        """Extract entities from query using LLM or fallback."""
        try:
            # Try LLM-based extraction first
            from shared.core.llm_client_v3 import EnhancedLLMClientV3

            llm_client = EnhancedLLMClientV3()
            prompt = f"""
            Extract named entities from the following query. Return only the entity names, one per line:
            
            Query: {query}
            
            Entities:
            """

            response = await llm_client.generate_text(prompt, max_tokens=100)
            entities = [
                line.strip() for line in response.strip().split("\n") if line.strip()
            ]

            return [
                {"name": entity, "type": "unknown", "confidence": 0.8}
                for entity in entities
            ]

        except Exception as e:
            self.logger.warning(f"LLM entity extraction failed: {e}, using fallback")
            return self._extract_common_entities(query)

    def _extract_common_entities(self, query: str) -> List[Dict[str, Any]]:
        """Extract common entities as fallback."""
        # Simple keyword-based extraction
        keywords = [
            "Python",
            "JavaScript",
            "React",
            "Vue",
            "Angular",
            "Node.js",
            "Machine Learning",
            "AI",
            "Artificial Intelligence",
            "Data Science",
            "Docker",
            "Kubernetes",
            "AWS",
            "Azure",
            "Google Cloud",
            "MongoDB",
            "PostgreSQL",
            "MySQL",
            "Redis",
            "Elasticsearch",
        ]

        entities = []
        query_lower = query.lower()

        for keyword in keywords:
            if keyword.lower() in query_lower:
                entities.append(
                    {"name": keyword, "type": "technology", "confidence": 0.9}
                )

        return entities


class RetrievalAgent(BaseAgent):
    """
    RetrievalAgent that performs hybrid search with token optimization and web crawl fallback.
    """

    def __init__(self, config: Dict[str, Any] = None):
        """Initialize the retrieval agent."""
        super().__init__(agent_id="retrieval_agent", agent_type=AgentType.RETRIEVAL)

        # Initialize shared utilities
        self.task_processor = AgentTaskProcessor(self.agent_id)
        self.logger = get_logger(f"{__name__}.{self.agent_id}")

        # Initialize components
        self.config = config or self._default_config()
        self.entity_extractor = EntityExtractor()

        # Initialize search clients
        self._initialize_search_clients()

        logger.info("✅ RetrievalAgent initialized successfully")

    def _default_config(self) -> Dict[str, Any]:
        """Default configuration for retrieval agent."""
        return {
            "vector_search": {
                "enabled": True,
                "top_k": 20,
                "similarity_threshold": 0.7,
            },
            "keyword_search": {"enabled": True, "top_k": 20},
            "knowledge_graph": {"enabled": True, "max_depth": 2},
            "web_search": {"enabled": True, "max_results": 10, "timeout": 30},
            "token_optimization": {
                "enabled": True,
                "max_tokens": 4000,
                "truncation_strategy": "smart",
            },
            "web_crawl_fallback": {
                "enabled": True,
                "max_pages": 5,
                "timeout": 30,
                "confidence_threshold": 0.6,
            },
        }

    def _initialize_search_clients(self):
        """Initialize search clients."""
        # Initialize vector search client
        self.vector_client = None
        try:
            from shared.core.services.vector_service import VectorService

            self.vector_client = VectorService()
        except Exception as e:
            self.logger.warning(f"Vector service not available: {e}")

        # Initialize keyword search client
        self.keyword_client = None
        try:
            from shared.core.services.search_service import SearchService

            self.keyword_client = SearchService()
        except Exception as e:
            self.logger.warning(f"Search service not available: {e}")

        # Initialize knowledge graph client
        self.graph_client = None
        try:
            from shared.core.services.knowledge_service import KnowledgeService

            self.graph_client = KnowledgeService()
        except Exception as e:
            self.logger.warning(f"Knowledge service not available: {e}")

        # Initialize web search client
        self.web_client = None
        try:
            from shared.core.services.web_service import WebService

            self.web_client = WebService()
        except Exception as e:
            self.logger.warning(f"Web service not available: {e}")

    @time_agent_function("retrieval_agent")
    async def process_task(
        self, task: Dict[str, Any], context: QueryContext
    ) -> Dict[str, Any]:
        """
        Process retrieval task using shared utilities.

        This method now uses the standardized workflow from AgentTaskProcessor
        to eliminate duplicate logic and ensure consistent behavior.
        """
        # Use shared task processor with validation
        result = await self.task_processor.process_task_with_workflow(
            task=task,
            context=context,
            processing_func=self._process_retrieval_task,
            validation_func=CommonValidators.validate_query_input,
            timeout_seconds=60,
        )

        # Convert TaskResult to standard response format
        return ResponseFormatter.format_agent_response(
            success=result.success,
            data=result.data,
            error=result.error,
            confidence=result.confidence,
            execution_time_ms=result.execution_time_ms,
            metadata=result.metadata,
        )

    async def _process_retrieval_task(
        self, task: Dict[str, Any], context: QueryContext
    ) -> Dict[str, Any]:
        """
        Process retrieval task with token optimization and web crawl fallback.

        Args:
            task: Task data containing query and search parameters
            context: Query context

        Returns:
            Dictionary with retrieval results
        """
        # Extract task parameters
        query = task.get("query", context.query)
        search_type = task.get("search_type", "hybrid")
        top_k = task.get("top_k", 20)
        max_tokens = task.get("max_tokens", 4000)
        enable_web_fallback = task.get("enable_web_fallback", True)
        web_fallback_timeout = task.get("web_fallback_timeout", 30)

        logger.info(f"Processing {search_type} search for query: {query[:50]}...")

        # Perform initial search based on type
        if search_type == "vector":
            result = await self.vector_search(query, top_k)
        elif search_type == "keyword":
            result = await self.keyword_search(query, top_k)
        elif search_type == "graph":
            entities = task.get("entities", [])
            result = await self.graph_search(entities, top_k)
        else:
            # Default to hybrid search
            entities = task.get("entities", None)
            result = await self.hybrid_retrieve(query, entities)

        # Check if web crawl fallback is needed
        if enable_web_fallback and self._should_use_web_crawl_fallback(
            result.documents, query
        ):
            logger.info("🔄 Local results insufficient, triggering web crawl fallback")

            try:
                # Perform web crawling with timeout
                web_result = await asyncio.wait_for(
                    self.web_crawl_fallback(
                        query, max_pages=5, timeout=web_fallback_timeout
                    ),
                    timeout=web_fallback_timeout,
                )

                # Merge web results with local results
                if web_result.documents:
                    logger.info(
                        f"✅ Web crawl returned {len(web_result.documents)} documents"
                    )

                    # Combine documents from both sources
                    all_documents = result.documents + web_result.documents

                    # Re-rank combined results
                    reranked_documents = await self._llm_rerank(
                        f"Query: {query}\n\nRank these documents by relevance:",
                        all_documents,
                    )

                    # Update result with merged documents
                    result = SearchResult(
                        documents=reranked_documents,
                        search_type=f"{result.search_type}+web_crawl",
                        query_time_ms=result.query_time_ms + web_result.query_time_ms,
                        total_hits=len(reranked_documents),
                        metadata={
                            **result.metadata,
                            "web_crawl_used": True,
                            "web_crawl_documents": len(web_result.documents),
                            "web_crawl_time_ms": web_result.query_time_ms,
                            "merged_sources": ["local", "web_crawl"],
                        },
                    )
                else:
                    logger.warning("⚠️ Web crawl returned no documents")

            except asyncio.TimeoutError:
                logger.warning("⚠️ Web crawl fallback timed out")
            except Exception as e:
                logger.error(f"❌ Web crawl fallback failed: {e}")

        # Optimize documents for token usage
        optimized_documents = self._optimize_documents_for_tokens(
            result.documents, max_tokens, query
        )

        # Calculate confidence based on result quality
        confidence = self._calculate_retrieval_confidence(optimized_documents)

        # Estimate token usage
        estimated_tokens = self._estimate_token_usage(query, optimized_documents)

        # Create standardized retrieval result
        retrieval_data = RetrievalResult(
            documents=[DocumentModel(**doc.to_dict()) for doc in optimized_documents],
            search_type=result.search_type,
            total_hits=result.total_hits,
            query_time_ms=result.query_time_ms,
            metadata={
                **result.metadata,
                "estimated_tokens": estimated_tokens,
                "confidence": confidence,
                "optimization_applied": True,
            },
        )

        return {
            "data": retrieval_data,
            "confidence": confidence,
            "metadata": {
                "search_type": search_type,
                "documents_found": len(optimized_documents),
                "estimated_tokens": estimated_tokens,
                "web_crawl_used": result.metadata.get("web_crawl_used", False),
            },
        }

    # ... rest of the existing methods remain unchanged ...
