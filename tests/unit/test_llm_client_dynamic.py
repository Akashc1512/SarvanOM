"""
Test suite for Dynamic LLM Client functionality.

This module provides comprehensive tests for the dynamic LLM client,
including query classification, provider selection, fallback logic,
and API response handling.

Test Coverage:
- Query classification accuracy
- Provider selection logic
- Fallback mechanisms
- API response handling
- Error handling and retries
- Performance monitoring
- Health checks

Authors: Universal Knowledge Platform Engineering Team
Version: 1.0.0 (2024-12-28)
"""

import pytest
import asyncio
import time
from unittest.mock import patch, AsyncMock, MagicMock
import warnings
import os

# Suppress warnings for cleaner test output
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=UserWarning)

# Load environment variables from .env file
try:
    from dotenv import load_dotenv

    load_dotenv()
except ImportError:
    pass  # Optional for tests

from shared.core.llm_client_dynamic import (
    DynamicLLMClient,
    LLMRequest,
    LLMResponse,
    LLMProvider,
    QueryType,
    ProviderConfig,
    OllamaProvider,
    HuggingFaceProvider,
    OpenAIProvider,
)


class TestDynamicLLMClient:
    """Test suite for Dynamic LLM Client functionality."""

    @pytest.fixture
    def client(self):
        """Create a test client with mocked providers."""
        # Mock environment variables to ensure all providers are available
        with patch.dict(
            os.environ,
            {
                "HUGGINGFACE_API_KEY": "test_hf_key",
                "OPENAI_API_KEY": "test_openai_key",
                "OLLAMA_BASE_URL": "http://localhost:11434",
            },
        ):
            # Mock the import that happens inside DynamicLLMClient.__init__
            with patch(
                "shared.core.query_classifier.QueryClassifier",
                side_effect=ImportError("QueryClassifier not available"),
            ):
                client = DynamicLLMClient()
                return client

    @pytest.mark.asyncio
    async def test_client_initialization(self, client):
        """Test client initialization and provider setup."""
        print("üîç Testing Client Initialization...")

        # Verify client was created
        assert isinstance(client, DynamicLLMClient)

        # Verify providers were set up
        assert len(client.providers) > 0

        # Verify provider stats were initialized
        assert all(provider in client.provider_stats for provider in LLMProvider)

        print("‚úÖ Client initialization test passed")

    @pytest.mark.asyncio
    async def test_query_classification_simple_factual(self, client):
        """Test query classification for simple factual queries."""
        print("üîç Testing Simple Factual Query Classification...")

        # Test simple factual query
        query = "What is the capital of France?"
        query_type = await client._classify_query(query)

        assert query_type == QueryType.SIMPLE_FACTUAL

        print("‚úÖ Simple factual query classification test passed")

    @pytest.mark.asyncio
    async def test_query_classification_research_synthesis(self, client):
        """Test query classification for research synthesis queries."""
        print("üîç Testing Research Synthesis Query Classification...")

        # Test research synthesis query
        query = "Analyze the impact of climate change on global agriculture"
        query_type = await client._classify_query(query)

        assert query_type == QueryType.RESEARCH_SYNTHESIS

        print("‚úÖ Research synthesis query classification test passed")

    @pytest.mark.asyncio
    async def test_query_classification_complex_reasoning(self, client):
        """Test query classification for complex reasoning queries."""
        print("üîç Testing Complex Reasoning Query Classification...")

        # Test complex reasoning query
        query = "Explain the philosophical implications of quantum mechanics"
        query_type = await client._classify_query(query)

        assert query_type == QueryType.COMPLEX_REASONING

        print("‚úÖ Complex reasoning query classification test passed")

    @pytest.mark.asyncio
    async def test_provider_selection_simple_factual(self, client):
        """Test provider selection for simple factual queries."""
        print("üîç Testing Provider Selection for Simple Factual Queries...")

        # Test that simple factual queries prefer Ollama
        query_type = QueryType.SIMPLE_FACTUAL
        selected_provider = client._select_provider(query_type)

        # Should prefer Ollama for simple factual queries
        assert selected_provider == LLMProvider.OLLAMA

        print("‚úÖ Provider selection for simple factual queries test passed")

    @pytest.mark.asyncio
    async def test_provider_selection_research_synthesis(self, client):
        """Test provider selection for research synthesis queries."""
        print("üîç Testing Provider Selection for Research Synthesis Queries...")

        # Test that research synthesis queries prefer HuggingFace
        query_type = QueryType.RESEARCH_SYNTHESIS
        selected_provider = client._select_provider(query_type)

        # Should prefer HuggingFace for research synthesis queries
        assert selected_provider == LLMProvider.HUGGINGFACE

        print("‚úÖ Provider selection for research synthesis queries test passed")

    @pytest.mark.asyncio
    async def test_provider_selection_complex_reasoning(self, client):
        """Test provider selection for complex reasoning queries."""
        print("üîç Testing Provider Selection for Complex Reasoning Queries...")

        # Test that complex reasoning queries prefer OpenAI
        query_type = QueryType.COMPLEX_REASONING
        selected_provider = client._select_provider(query_type)

        # Should prefer OpenAI for complex reasoning queries
        assert selected_provider == LLMProvider.OPENAI

        print("‚úÖ Provider selection for complex reasoning queries test passed")

    @pytest.mark.asyncio
    async def test_provider_fallback_logic(self, client):
        """Test provider fallback logic when primary provider is unavailable."""
        print("üîç Testing Provider Fallback Logic...")

        # Mock providers with different availability
        mock_ollama = AsyncMock()
        mock_hf = AsyncMock()
        mock_openai = AsyncMock()

        # Make Ollama unhealthy, others healthy
        mock_ollama.health_check.return_value = False
        mock_hf.health_check.return_value = True
        mock_openai.health_check.return_value = True

        # Mock Ollama to fail generation, HuggingFace to succeed
        mock_ollama.generate_text.side_effect = Exception("Ollama failed")
        mock_hf.generate_text.return_value = LLMResponse(
            content="HuggingFace fallback response",
            provider=LLMProvider.HUGGINGFACE,
            model="microsoft/DialoGPT-medium",
            response_time_ms=2000.0,
        )

        client.providers = {
            LLMProvider.OLLAMA: mock_ollama,
            LLMProvider.HUGGINGFACE: mock_hf,
            LLMProvider.OPENAI: mock_openai,
        }

        # Test fallback for simple factual query
        query = "What is the capital of France?"

        # Mock query classification
        with patch.object(
            client, "_classify_query", return_value=QueryType.SIMPLE_FACTUAL
        ):
            response = await client.dispatch(query)

        # Verify fallback was used
        assert isinstance(response, LLMResponse)
        assert response.provider == LLMProvider.HUGGINGFACE
        assert "HuggingFace fallback response" in response.content

        print("‚úÖ Provider fallback logic test passed")

    @pytest.mark.asyncio
    async def test_dispatch_simple_factual_query(self, client):
        """Test dispatching a simple factual query."""
        print("üîç Testing Dispatch for Simple Factual Query...")

        query = "What is the capital of France?"
        context = "Geography facts"

        # Mock the providers to return successful responses
        mock_ollama = AsyncMock()
        mock_ollama.generate_text.return_value = LLMResponse(
            content="Ollama response for simple factual query",
            provider=LLMProvider.OLLAMA,
            model="llama2:7b",
            response_time_ms=1500.0,
        )
        mock_ollama.health_check.return_value = True

        client.providers = {LLMProvider.OLLAMA: mock_ollama}

        # Mock query classification
        with patch.object(
            client, "_classify_query", return_value=QueryType.SIMPLE_FACTUAL
        ):
            response = await client.dispatch(query, context)

        # Verify response structure
        assert isinstance(response, LLMResponse)
        assert response.provider == LLMProvider.OLLAMA
        assert "Ollama response" in response.content
        assert response.response_time_ms > 0

        print("‚úÖ Dispatch for simple factual query test passed")

    @pytest.mark.asyncio
    async def test_dispatch_research_synthesis_query(self, client):
        """Test dispatching a research synthesis query."""
        print("üîç Testing Dispatch for Research Synthesis Query...")

        query = "Analyze the impact of climate change on agriculture"
        context = "Climate research data"

        # Mock the providers to return successful responses
        mock_hf = AsyncMock()
        mock_hf.generate_text.return_value = LLMResponse(
            content="HuggingFace response for research synthesis",
            provider=LLMProvider.HUGGINGFACE,
            model="microsoft/DialoGPT-medium",
            response_time_ms=2500.0,
        )
        mock_hf.health_check.return_value = True

        client.providers = {LLMProvider.HUGGINGFACE: mock_hf}

        # Mock query classification
        with patch.object(
            client, "_classify_query", return_value=QueryType.RESEARCH_SYNTHESIS
        ):
            response = await client.dispatch(query, context)

        # Verify response structure
        assert isinstance(response, LLMResponse)
        assert response.provider == LLMProvider.HUGGINGFACE
        assert "HuggingFace response" in response.content
        assert response.response_time_ms > 0

        print("‚úÖ Dispatch for research synthesis query test passed")

    @pytest.mark.asyncio
    async def test_dispatch_complex_reasoning_query(self, client):
        """Test dispatching a complex reasoning query."""
        print("üîç Testing Dispatch for Complex Reasoning Query...")

        query = "Explain the philosophical implications of quantum mechanics"
        context = "Quantum physics and philosophy"

        # Mock the providers to return successful responses
        mock_openai = AsyncMock()
        mock_openai.generate_text.return_value = LLMResponse(
            content="OpenAI response for complex reasoning",
            provider=LLMProvider.OPENAI,
            model="gpt-3.5-turbo",
            response_time_ms=3000.0,
        )
        mock_openai.health_check.return_value = True

        client.providers = {LLMProvider.OPENAI: mock_openai}

        # Mock query classification
        with patch.object(
            client, "_classify_query", return_value=QueryType.COMPLEX_REASONING
        ):
            response = await client.dispatch(query, context)

        # Verify response structure
        assert isinstance(response, LLMResponse)
        assert response.provider == LLMProvider.OPENAI
        assert "OpenAI response" in response.content
        assert response.response_time_ms > 0

        print("‚úÖ Dispatch for complex reasoning query test passed")

    @pytest.mark.asyncio
    async def test_dispatch_with_fallback(self, client):
        """Test dispatch with fallback when primary provider fails."""
        print("üîç Testing Dispatch with Fallback...")

        query = "What is the capital of France?"

        # Mock the providers
        mock_ollama = AsyncMock()
        mock_ollama.generate_text.side_effect = Exception("Ollama failed")
        mock_ollama.health_check.return_value = True

        mock_hf = AsyncMock()
        mock_hf.generate_text.return_value = LLMResponse(
            content="HuggingFace fallback response",
            provider=LLMProvider.HUGGINGFACE,
            model="microsoft/DialoGPT-medium",
            response_time_ms=2000.0,
        )
        mock_hf.health_check.return_value = True

        client.providers = {
            LLMProvider.OLLAMA: mock_ollama,
            LLMProvider.HUGGINGFACE: mock_hf,
        }

        # Mock query classification
        with patch.object(
            client, "_classify_query", return_value=QueryType.SIMPLE_FACTUAL
        ):
            response = await client.dispatch(query)

        # Verify fallback was used
        assert isinstance(response, LLMResponse)
        assert response.provider == LLMProvider.HUGGINGFACE
        assert "HuggingFace fallback response" in response.content

        print("‚úÖ Dispatch with fallback test passed")

    @pytest.mark.asyncio
    async def test_dispatch_all_providers_fail(self, client):
        """Test dispatch when all providers fail."""
        print("üîç Testing Dispatch with All Providers Failing...")

        query = "What is the capital of France?"

        # Mock all providers to fail
        mock_provider = AsyncMock()
        mock_provider.generate_text.side_effect = Exception("Provider failed")
        mock_provider.health_check.return_value = True

        client.providers = {LLMProvider.OLLAMA: mock_provider}

        # Mock query classification
        with patch.object(
            client, "_classify_query", return_value=QueryType.SIMPLE_FACTUAL
        ):
            with pytest.raises(Exception) as exc_info:
                await client.dispatch(query)

            # Check that the error message contains the expected text
            # The RetryError wraps the original exception, so we need to check the cause
            error_str = str(exc_info.value)
            if "RetryError" in error_str:
                # For RetryError, check the original exception
                original_exception = (
                    exc_info.value.args[0] if exc_info.value.args else None
                )
                if original_exception and hasattr(original_exception, "exception"):
                    error_str = str(original_exception.exception())
                else:
                    # Try to extract the original error from the RetryError message
                    error_str = str(exc_info.value)

            assert (
                "All LLM providers failed" in error_str
                or "Provider failed" in error_str
            )

        print("‚úÖ Dispatch with all providers failing test passed")

    @pytest.mark.asyncio
    async def test_generate_text_convenience_method(self, client):
        """Test the generate_text convenience method."""
        print("üîç Testing Generate Text Convenience Method...")

        prompt = "What is the capital of France?"

        # Mock the dispatch method
        with patch.object(client, "dispatch") as mock_dispatch:
            mock_response = LLMResponse(
                content="Ollama response for simple factual query",
                provider=LLMProvider.OLLAMA,
                model="llama2:7b",
                response_time_ms=1500.0,
            )
            mock_dispatch.return_value = mock_response

            response = await client.generate_text(prompt)

        # Verify response is a string
        assert isinstance(response, str)
        assert "Ollama response" in response

        print("‚úÖ Generate text convenience method test passed")

    @pytest.mark.asyncio
    async def test_provider_statistics_tracking(self, client):
        """Test that provider statistics are properly tracked."""
        print("üîç Testing Provider Statistics Tracking...")

        query = "What is the capital of France?"

        # Mock the providers
        mock_provider = AsyncMock()
        mock_provider.generate_text.return_value = LLMResponse(
            content="Test response",
            provider=LLMProvider.OLLAMA,
            model="llama2:7b",
            response_time_ms=1000.0,
        )
        mock_provider.health_check.return_value = True

        client.providers = {LLMProvider.OLLAMA: mock_provider}

        # Mock query classification
        with patch.object(
            client, "_classify_query", return_value=QueryType.SIMPLE_FACTUAL
        ):
            await client.dispatch(query)

        # Verify statistics were updated
        stats = client.get_provider_stats()

        assert stats["provider_stats"][LLMProvider.OLLAMA]["calls"] == 1
        assert stats["provider_stats"][LLMProvider.OLLAMA]["success"] == 1
        assert stats["provider_stats"][LLMProvider.OLLAMA]["avg_time"] > 0

        assert stats["total_calls"] == 1
        assert stats["total_success"] == 1

        print("‚úÖ Provider statistics tracking test passed")

    @pytest.mark.asyncio
    async def test_health_check_all_providers(self, client):
        """Test health check for all providers."""
        print("üîç Testing Health Check for All Providers...")

        # Mock providers
        mock_ollama = AsyncMock()
        mock_ollama.health_check.return_value = True
        mock_ollama.get_provider_info.return_value = {
            "provider": "ollama",
            "base_url": "http://localhost:11434",
            "default_model": "llama2:7b",
            "is_local": True,
            "cost_per_token": 0.0,
        }

        mock_hf = AsyncMock()
        mock_hf.health_check.return_value = True
        mock_hf.get_provider_info.return_value = {
            "provider": "huggingface",
            "base_url": "https://api-inference.huggingface.co",
            "default_model": "microsoft/DialoGPT-medium",
            "is_local": False,
            "cost_per_token": 0.0,
        }

        mock_openai = AsyncMock()
        mock_openai.health_check.return_value = True
        mock_openai.get_provider_info.return_value = {
            "provider": "openai",
            "base_url": "https://api.openai.com/v1",
            "default_model": "gpt-3.5-turbo",
            "is_local": False,
            "cost_per_token": 0.002,
        }

        client.providers = {
            LLMProvider.OLLAMA: mock_ollama,
            LLMProvider.HUGGINGFACE: mock_hf,
            LLMProvider.OPENAI: mock_openai,
        }

        health_status = await client.health_check()

        # Verify health status structure
        assert isinstance(health_status, dict)
        assert LLMProvider.OLLAMA.value in health_status
        assert LLMProvider.HUGGINGFACE.value in health_status
        assert LLMProvider.OPENAI.value in health_status

        # Verify all providers are healthy
        for provider_status in health_status.values():
            assert provider_status["healthy"] == True
            assert "info" in provider_status

        print("‚úÖ Health check for all providers test passed")

    @pytest.mark.asyncio
    async def test_health_check_with_failing_provider(self, client):
        """Test health check when some providers are failing."""
        print("üîç Testing Health Check with Failing Provider...")

        # Mock providers with one failing
        mock_healthy = AsyncMock()
        mock_healthy.health_check.return_value = True
        mock_healthy.get_provider_info.return_value = {"provider": "healthy"}

        mock_failing = AsyncMock()
        mock_failing.health_check.side_effect = Exception("Connection failed")

        client.providers = {
            LLMProvider.OLLAMA: mock_failing,
            LLMProvider.HUGGINGFACE: mock_healthy,
        }

        health_status = await client.health_check()

        # Verify Ollama is marked as unhealthy
        assert health_status[LLMProvider.OLLAMA.value]["healthy"] == False
        assert "error" in health_status[LLMProvider.OLLAMA.value]

        # Verify others are healthy
        assert health_status[LLMProvider.HUGGINGFACE.value]["healthy"] == True

        print("‚úÖ Health check with failing provider test passed")

    @pytest.mark.asyncio
    async def test_request_structure_validation(self, client):
        """Test that request structure is properly validated and formatted."""
        print("üîç Testing Request Structure Validation...")

        query = "What is the capital of France?"
        context = "Geography facts"
        system_message = "You are a helpful assistant."

        # Mock the providers
        mock_provider = AsyncMock()
        mock_provider.generate_text.return_value = LLMResponse(
            content="Test response",
            provider=LLMProvider.OLLAMA,
            model="llama2:7b",
            response_time_ms=1000.0,
        )
        mock_provider.health_check.return_value = True

        client.providers = {LLMProvider.OLLAMA: mock_provider}

        # Mock query classification
        with patch.object(
            client, "_classify_query", return_value=QueryType.SIMPLE_FACTUAL
        ):
            response = await client.dispatch(
                query=query,
                context=context,
                system_message=system_message,
                max_tokens=500,
                temperature=0.3,
            )

        # Verify the request was properly formatted
        call_args = mock_provider.generate_text.call_args
        request = call_args[0][0]  # First argument is the request

        assert isinstance(request, LLMRequest)
        assert request.query == query
        assert request.context == context
        assert request.system_message == system_message
        assert request.max_tokens == 500
        assert request.temperature == 0.3

        print("‚úÖ Request structure validation test passed")

    @pytest.mark.asyncio
    async def test_response_structure_validation(self, client):
        """Test that response structure is properly formatted."""
        print("üîç Testing Response Structure Validation...")

        query = "What is the capital of France?"

        # Mock the providers
        mock_provider = AsyncMock()
        mock_provider.generate_text.return_value = LLMResponse(
            content="Test response content",
            provider=LLMProvider.OLLAMA,
            model="llama2:7b",
            response_time_ms=1000.0,
            metadata={"test": "data"},
        )
        mock_provider.health_check.return_value = True

        client.providers = {LLMProvider.OLLAMA: mock_provider}

        # Mock query classification
        with patch.object(
            client, "_classify_query", return_value=QueryType.SIMPLE_FACTUAL
        ):
            response = await client.dispatch(query)

        # Verify response structure
        assert isinstance(response, LLMResponse)
        assert isinstance(response.content, str)
        assert isinstance(response.provider, LLMProvider)
        assert isinstance(response.model, str)
        assert isinstance(response.response_time_ms, float)
        assert response.response_time_ms > 0

        # Verify metadata is present
        assert isinstance(response.metadata, dict)

        print("‚úÖ Response structure validation test passed")

    @pytest.mark.asyncio
    async def test_convenience_functions(self):
        """Test the convenience functions for easy usage."""
        print("üîç Testing Convenience Functions...")

        from shared.core.llm_client_dynamic import (
            get_dynamic_llm_client,
            dispatch_query,
            generate_text,
        )

        # Test get_dynamic_llm_client
        with patch(
            "shared.core.query_classifier.QueryClassifier",
            side_effect=ImportError("QueryClassifier not available"),
        ):
            with patch.dict(
                os.environ,
                {
                    "HUGGINGFACE_API_KEY": "test_hf_key",
                    "OPENAI_API_KEY": "test_openai_key",
                    "OLLAMA_BASE_URL": "http://localhost:11434",
                },
            ):
                client = get_dynamic_llm_client()
                assert isinstance(client, DynamicLLMClient)

        # Test dispatch_query with mocked providers
        with patch(
            "shared.core.query_classifier.QueryClassifier",
            side_effect=ImportError("QueryClassifier not available"),
        ):
            with patch.dict(
                os.environ,
                {
                    "HUGGINGFACE_API_KEY": "test_hf_key",
                    "OPENAI_API_KEY": "test_openai_key",
                    "OLLAMA_BASE_URL": "http://localhost:11434",
                },
            ):
                with patch.object(DynamicLLMClient, "dispatch") as mock_dispatch:
                    mock_response = LLMResponse(
                        content="Test response",
                        provider=LLMProvider.OLLAMA,
                        model="llama2:7b",
                        response_time_ms=1000.0,
                    )
                    mock_dispatch.return_value = mock_response

                    response = await dispatch_query("Test query")
                    assert response == mock_response

        # Test generate_text with mocked providers
        with patch(
            "shared.core.query_classifier.QueryClassifier",
            side_effect=ImportError("QueryClassifier not available"),
        ):
            with patch.dict(
                os.environ,
                {
                    "HUGGINGFACE_API_KEY": "test_hf_key",
                    "OPENAI_API_KEY": "test_openai_key",
                    "OLLAMA_BASE_URL": "http://localhost:11434",
                },
            ):
                with patch.object(DynamicLLMClient, "generate_text") as mock_generate:
                    mock_generate.return_value = "Test text"

                    text = await generate_text("Test prompt")
                    assert text == "Test text"

        print("‚úÖ Convenience functions test passed")


@pytest.mark.asyncio
async def test_llm_client_dynamic_integration():
    """Integration test for the dynamic LLM client."""
    print("üîç Testing Dynamic LLM Client Integration...")

    # Test the complete flow
    try:
        from shared.core.llm_client_dynamic import get_dynamic_llm_client

        with patch(
            "shared.core.query_classifier.QueryClassifier",
            side_effect=ImportError("QueryClassifier not available"),
        ):
            with patch.dict(
                os.environ,
                {
                    "HUGGINGFACE_API_KEY": "test_hf_key",
                    "OPENAI_API_KEY": "test_openai_key",
                    "OLLAMA_BASE_URL": "http://localhost:11434",
                },
            ):
                client = get_dynamic_llm_client()

                # Test with a simple query
                query = "What is the capital of France?"

                # This will use mock providers in test environment
                response = await client.dispatch(query)

                assert isinstance(response, LLMResponse)
                assert len(response.content) > 0

                print("‚úÖ Dynamic LLM client integration test passed")

    except Exception as e:
        print(f"‚ö†Ô∏è Integration test failed (expected in test environment): {e}")
        # This is expected in test environment without real providers


if __name__ == "__main__":
    # Run the tests
    pytest.main([__file__, "-v", "--tb=short"])
